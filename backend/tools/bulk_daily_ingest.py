"""
Bulk Daily Report Ingestion Script

backend/Data/mock_reports/daily í´ë”ì˜ ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì„œ
PostgreSQLì˜ daily_reports í…Œì´ë¸”ì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python backend/tools/bulk_daily_ingest.py
"""
import sys
import os
import json
import re
from pathlib import Path
from datetime import datetime, date
from typing import List, Dict, Any, Optional

# Windows ì½˜ì†” UTF-8 ì„¤ì •
if sys.platform == 'win32':
    os.system('chcp 65001 >nul 2>&1')
    sys.stdout.reconfigure(encoding='utf-8') if hasattr(sys.stdout, 'reconfigure') else None

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
backend_dir = Path(__file__).resolve().parent.parent  # backend/
project_root = backend_dir.parent  # Virtual-Assistant ë£¨íŠ¸
sys.path.insert(0, str(project_root))  # tools ëª¨ë“ˆ importë¥¼ ìœ„í•´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(backend_dir))  # app ëª¨ë“ˆ importë¥¼ ìœ„í•´ backend ì¶”ê°€

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (config ì„¤ì •ì„ ìœ„í•´ í•„ìš”)
from dotenv import load_dotenv
load_dotenv(backend_dir / ".env")
report_env = backend_dir / ".env.report"
if report_env.exists():
    load_dotenv(report_env, override=False)

from app.infrastructure.database.session import SessionLocal
from app.domain.report.daily.repository import DailyReportRepository
from app.domain.report.daily.schemas import DailyReportCreate
from app.domain.report.core.canonical_models import CanonicalReport, CanonicalDaily, DetailTask
import uuid


def parse_time_range(time_str: str) -> tuple[Optional[str], Optional[str]]:
    """
    ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ (start, end) íŠœí”Œ ë°˜í™˜
    
    ì˜ˆ: "09:00 - 10:00" -> ("09:00", "10:00")
    
    Args:
        time_str: ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´
        
    Returns:
        (time_start, time_end) íŠœí”Œ
    """
    if not time_str or time_str.strip() == "":
        return (None, None)
    
    # "09:00 - 10:00" íŒ¨í„´ ë§¤ì¹­
    match = re.match(r'(\d{1,2}:\d{2})\s*-\s*(\d{1,2}:\d{2})', time_str.strip())
    if match:
        return (match.group(1), match.group(2))
    
    # ë‹¨ì¼ ì‹œê°„ë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: "09:00")
    match = re.match(r'(\d{1,2}:\d{2})', time_str.strip())
    if match:
        return (match.group(1), None)
    
    return (None, None)


def parse_date(date_str: str) -> date:
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ date ê°ì²´ë¡œ ë³€í™˜
    
    ì˜ˆ: "2025-01-02" -> date(2025, 1, 2)
    
    Args:
        date_str: ë‚ ì§œ ë¬¸ìì—´ (YYYY-MM-DD)
        
    Returns:
        date ê°ì²´
    """
    try:
        return datetime.strptime(date_str, "%Y-%m-%d").date()
    except ValueError as e:
        raise ValueError(f"ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {date_str}. YYYY-MM-DD í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ({e})")


def convert_to_canonical_report(raw_json: Dict[str, Any], owner: str) -> CanonicalReport:
    """
    Raw JSONì„ CanonicalReportë¡œ ë³€í™˜
    
    Args:
        raw_json: ì›ë³¸ JSON ë”•ì…”ë„ˆë¦¬
        owner: í˜¸ì¶œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì „ë‹¬ëœ owner (ë¬¸ì„œì—ì„œ ì½ì§€ ì•ŠìŒ)
        
    Returns:
        CanonicalReport ê°ì²´
    """
    owner = (owner or "").strip()
    if not owner:
        raise ValueError("owner is required for ingestion (cannot be read from document).")

    # 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    ì‘ì„±ì¼ì = raw_json["ìƒë‹¨ì •ë³´"]["ì‘ì„±ì¼ì"]
    
    period_date = parse_date(ì‘ì„±ì¼ì)
    
    # 2. í—¤ë” ì •ë³´
    header = {
        "ì‘ì„±ì¼ì": ì‘ì„±ì¼ì,
        "ì„±ëª…": owner
    }
    
    # 3. todo_tasks (ê¸ˆì¼_ì§„í–‰_ì—…ë¬´)
    todo_tasks = []
    ê¸ˆì¼ì§„í–‰ì—…ë¬´ = raw_json.get("ê¸ˆì¼_ì§„í–‰_ì—…ë¬´", "")
    if ê¸ˆì¼ì§„í–‰ì—…ë¬´:
        if isinstance(ê¸ˆì¼ì§„í–‰ì—…ë¬´, list):
            todo_tasks = ê¸ˆì¼ì§„í–‰ì—…ë¬´
        else:
            todo_tasks = [ê¸ˆì¼ì§„í–‰ì—…ë¬´] if ê¸ˆì¼ì§„í–‰ì—…ë¬´.strip() else []
    
    # 4. detail_tasks (ì„¸ë¶€ì—…ë¬´)
    detail_tasks = []
    ì„¸ë¶€ì—…ë¬´ = raw_json.get("ì„¸ë¶€ì—…ë¬´", [])
    for task_data in ì„¸ë¶€ì—…ë¬´:
        ì—…ë¬´ë‚´ìš© = task_data.get("ì—…ë¬´ë‚´ìš©", "")
        if not ì—…ë¬´ë‚´ìš© or not ì—…ë¬´ë‚´ìš©.strip():
            continue
        
        time_str = task_data.get("ì‹œê°„", "")
        time_start, time_end = parse_time_range(time_str)
        
        detail_task = DetailTask(
            time_start=time_start,
            time_end=time_end,
            text=ì—…ë¬´ë‚´ìš©,
            note=task_data.get("ë¹„ê³ ", "")
        )
        detail_tasks.append(detail_task)
    
    # 5. pending (ë¯¸ì¢…ê²°_ì—…ë¬´ì‚¬í•­)
    pending = []
    ë¯¸ì¢…ê²° = raw_json.get("ë¯¸ì¢…ê²°_ì—…ë¬´ì‚¬í•­", "")
    if ë¯¸ì¢…ê²°:
        if isinstance(ë¯¸ì¢…ê²°, list):
            pending = ë¯¸ì¢…ê²°
        else:
            pending = [ë¯¸ì¢…ê²°] if ë¯¸ì¢…ê²°.strip() else []
    
    # 6. plans (ìµì¼_ì—…ë¬´ê³„íš)
    plans = []
    ìµì¼ê³„íš = raw_json.get("ìµì¼_ì—…ë¬´ê³„íš", "")
    if ìµì¼ê³„íš:
        if isinstance(ìµì¼ê³„íš, list):
            plans = ìµì¼ê³„íš
        else:
            plans = [ìµì¼ê³„íš] if ìµì¼ê³„íš.strip() else []
    
    # 7. notes (íŠ¹ì´ì‚¬í•­) - notesì™€ summary ëª¨ë‘ ì„¤ì •
    notes = raw_json.get("íŠ¹ì´ì‚¬í•­", "") or ""
    summary = raw_json.get("íŠ¹ì´ì‚¬í•­", "") or ""  # íŠ¹ì´ì‚¬í•­ì„ summaryë¡œë„ ì‚¬ìš©
    
    # 8. CanonicalDaily ìƒì„±
    canonical_daily = CanonicalDaily(
        header=header,
        todo_tasks=todo_tasks,
        detail_tasks=detail_tasks,
        pending=pending,
        plans=plans,
        notes=notes,
        summary=summary
    )
    
    # 9. CanonicalReport ìƒì„±
    report = CanonicalReport(
        report_id=str(uuid.uuid4()),
        report_type="daily",
        owner=owner,  # owner íŒŒë¼ë¯¸í„° ì‚¬ìš©
        period_start=period_date,
        period_end=period_date,
        daily=canonical_daily
    )
    
    return report


def read_json_objects_from_file(file_path: Path) -> List[Dict[str, Any]]:
    """
    txt íŒŒì¼ì—ì„œ ì—¬ëŸ¬ JSON ê°ì²´ë¥¼ ì½ì–´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    
    ê° JSON ê°ì²´ëŠ” ë¹ˆ ì¤„ë¡œ êµ¬ë¶„ë¨
    
    Args:
        file_path: txt íŒŒì¼ ê²½ë¡œ
        
    Returns:
        JSON ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    json_objects = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ë¹ˆ ì¤„ë¡œ ë¶„ë¦¬ëœ JSON ê°ì²´ë“¤ì„ ì¶”ì¶œ
        # ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” íŒ¨í„´ ì°¾ê¸°
        json_texts = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
        
        for json_text in json_texts:
            try:
                obj = json.loads(json_text)
                json_objects.append(obj)
            except json.JSONDecodeError as e:
                print(f"âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜ ({file_path.name}): {e}")
                continue
    
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({file_path}): {e}")
    
    return json_objects


def find_all_txt_files(base_dir: Path, year: Optional[int] = None, month: Optional[int] = None) -> List[Path]:
    """
    base_dir í•˜ìœ„ì˜ txt íŒŒì¼ ì°¾ê¸° (ë‚ ì§œ í•„í„°ë§ ì˜µì…˜)
    
    Args:
        base_dir: ê¸°ë³¸ ë””ë ‰í† ë¦¬
        year: í•„í„°ë§í•  ì—°ë„ (Noneì´ë©´ ëª¨ë“  ì—°ë„)
        month: í•„í„°ë§í•  ì›” (Noneì´ë©´ ëª¨ë“  ì›”, ì˜ˆ: 11 = 11ì›”)
        
    Returns:
        txt íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    all_files = list(base_dir.rglob("*.txt"))
    
    # ë‚ ì§œ í•„í„°ë§ì´ ì—†ìœ¼ë©´ ëª¨ë“  íŒŒì¼ ë°˜í™˜
    if year is None and month is None:
        return sorted(all_files)
    
    # ë‚ ì§œ í•„í„°ë§
    filtered_files = []
    for file_path in all_files:
        filename = file_path.stem  # í™•ì¥ì ì œê±°
        
        try:
            # YYYY-MM-DD í˜•ì‹ íŒŒì‹±
            parts = filename.split('-')
            if len(parts) >= 3:
                file_year = int(parts[0])
                file_month = int(parts[1])
                
                # í•„í„°ë§ ì¡°ê±´ í™•ì¸
                if year is not None and file_year != year:
                    continue
                if month is not None and file_month != month:
                    continue
                
                filtered_files.append(file_path)
        except (ValueError, IndexError):
            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•œ íŒŒì¼ì€ ì œì™¸
            continue
    
    return sorted(filtered_files)


def preview_files(year: Optional[int] = None, month: Optional[int] = None):
    """
    íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (DB ì €ì¥ ì—†ì´)
    
    Args:
        year: í•„í„°ë§í•  ì—°ë„ (Noneì´ë©´ ëª¨ë“  ì—°ë„)
        month: í•„í„°ë§í•  ì›” (Noneì´ë©´ ëª¨ë“  ì›”, ì˜ˆ: 11 = 11ì›”)
    """
    print("=" * 70)
    print("ğŸ‘€ Daily Report íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°")
    if year or month:
        filter_msg = []
        if year:
            filter_msg.append(f"{year}ë…„")
        if month:
            filter_msg.append(f"{month}ì›”")
        print(f"í•„í„°: {' '.join(filter_msg)}")
    print("=" * 70)
    
    # 1. ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = backend_dir / "Data" / "mock_reports" / "daily"
    
    if not base_dir.exists():
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        return
    
    print(f"\nğŸ“ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {base_dir}")
    
    # 2. ëª¨ë“  txt íŒŒì¼ ì°¾ê¸°
    txt_files = find_all_txt_files(base_dir, year=year, month=month)
    print(f"ğŸ“„ ë°œê²¬ëœ txt íŒŒì¼: {len(txt_files)}ê°œ\n")
    
    if not txt_files:
        print("âš ï¸  txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3. ê° í´ë”ë³„ íŒŒì¼ í†µê³„
    folder_stats = {}
    total_json_count = 0
    
    for file_path in txt_files:
        folder_name = file_path.parent.name
        
        # JSON ê°ì²´ ìˆ˜ í™•ì¸
        json_objects = read_json_objects_from_file(file_path)
        json_count = len(json_objects)
        total_json_count += json_count
        
        if folder_name not in folder_stats:
            folder_stats[folder_name] = {
                "files": [],
                "total_json": 0
            }
        
        folder_stats[folder_name]["files"].append({
            "name": file_path.name,
            "json_count": json_count
        })
        folder_stats[folder_name]["total_json"] += json_count
    
    # 4. í´ë”ë³„ ì¶œë ¥
    print("ğŸ“‚ í´ë”ë³„ íŒŒì¼ ëª©ë¡:\n")
    
    for folder_name in sorted(folder_stats.keys()):
        stats = folder_stats[folder_name]
        print(f"ğŸ“ {folder_name}")
        print(f"   â”œâ”€ íŒŒì¼ ìˆ˜: {len(stats['files'])}ê°œ")
        print(f"   â”œâ”€ ë³´ê³ ì„œ ìˆ˜: {stats['total_json']}ê°œ")
        print(f"   â””â”€ íŒŒì¼ ëª©ë¡:")
        
        for file_info in stats["files"]:
            print(f"      â”œâ”€ {file_info['name']} ({file_info['json_count']}ê°œ)")
        
        print()
    
    # 5. ì „ì²´ í†µê³„
    print("=" * 70)
    print("ğŸ“Š ì „ì²´ í†µê³„:")
    print(f"   â”œâ”€ í´ë” ìˆ˜: {len(folder_stats)}ê°œ")
    print(f"   â”œâ”€ íŒŒì¼ ìˆ˜: {len(txt_files)}ê°œ")
    print(f"   â””â”€ ì´ ë³´ê³ ì„œ ìˆ˜: {total_json_count}ê°œ")
    print("=" * 70)
    
    # 6. ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°
    print("\nğŸ“– ì²« ë²ˆì§¸ íŒŒì¼ ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°:\n")
    
    if txt_files:
        first_file = txt_files[0]
        json_objects = read_json_objects_from_file(first_file)
        
        if json_objects:
            first_json = json_objects[0]
            print(f"íŒŒì¼: {first_file.name}")
            print(f"ì‘ì„±ì¼ì: {first_json.get('ìƒë‹¨ì •ë³´', {}).get('ì‘ì„±ì¼ì', 'N/A')}")
            print(f"ì„±ëª…: {first_json.get('ìƒë‹¨ì •ë³´', {}).get('ì„±ëª…', 'N/A')}")
            print(f"ì„¸ë¶€ì—…ë¬´ ìˆ˜: {len(first_json.get('ì„¸ë¶€ì—…ë¬´', []))}ê°œ")
            print(f"ê¸ˆì¼ ì§„í–‰ ì—…ë¬´: {first_json.get('ê¸ˆì¼_ì§„í–‰_ì—…ë¬´', 'N/A')[:50]}...")
    
    print("\n" + "=" * 70)
    print("âœ… ë¯¸ë¦¬ë³´ê¸° ì™„ë£Œ!")
    print("\nì‹¤í–‰í•˜ë ¤ë©´:")
    print("  python backend/tools/bulk_daily_ingest.py")
    print("=" * 70)


def bulk_ingest_daily_reports(year: Optional[int] = None, month: Optional[int] = None, owner: Optional[str] = None):
    """
    ë©”ì¸ í•¨ìˆ˜: ì¼ì¼ë³´ê³ ì„œë¥¼ DBì— ì €ì¥
    
    Args:
        year: í•„í„°ë§í•  ì—°ë„ (Noneì´ë©´ ëª¨ë“  ì—°ë„)
        month: í•„í„°ë§í•  ì›” (Noneì´ë©´ ëª¨ë“  ì›”, ì˜ˆ: 11 = 11ì›”)
    """
    print("=" * 70)
    print("ğŸ“Š ì¼ì¼ë³´ê³ ì„œ Bulk Ingestion ì‹œì‘")
    if year or month:
        filter_msg = []
        if year:
            filter_msg.append(f"{year}ë…„")
        if month:
            filter_msg.append(f"{month}ì›”")
        print(f"í•„í„°: {' '.join(filter_msg)}")
    print("=" * 70)

    owner = (owner or os.getenv("REPORT_OWNER") or "").strip()
    if not owner:
        raise ValueError("owner is required (set --owner or REPORT_OWNER env).")
    
    # 1. ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = backend_dir / "Data" / "mock_reports" / "daily"
    
    if not base_dir.exists():
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        return
    
    print(f"\nğŸ“ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {base_dir}")
    
    # 2. txt íŒŒì¼ ì°¾ê¸° (ë‚ ì§œ í•„í„°ë§ ì ìš©)
    txt_files = find_all_txt_files(base_dir, year=year, month=month)
    print(f"ğŸ“„ ë°œê²¬ëœ txt íŒŒì¼: {len(txt_files)}ê°œ")
    if year or month:
        print(f"   (í•„í„°: {year or 'ëª¨ë“  ì—°ë„'}ë…„ {month or 'ëª¨ë“  ì›”'}ì›”)")
    
    if not txt_files:
        print("âš ï¸  txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3. DB ì„¸ì…˜ ìƒì„±
    db = SessionLocal()
    
    # í†µê³„
    total_reports = 0
    created_count = 0
    updated_count = 0
    error_count = 0
    
    try:
        # 4. ê° íŒŒì¼ ì²˜ë¦¬ (ì§„í–‰ë¥  í‘œì‹œ)
        total_files = len(txt_files)
        print(f"   ğŸ“„ ì´ {total_files}ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...", end="", flush=True)
        
        for file_idx, file_path in enumerate(txt_files, 1):
            # 4-1. íŒŒì¼ì—ì„œ JSON ê°ì²´ë“¤ ì½ê¸°
            json_objects = read_json_objects_from_file(file_path)
            
            # 4-2. ê° JSON ê°ì²´ë¥¼ CanonicalReportë¡œ ë³€í™˜ í›„ DB ì €ì¥
            for json_obj in json_objects:
                try:
                    # CanonicalReport ë³€í™˜
                    canonical_report = convert_to_canonical_report(json_obj, owner=owner)
                    
                    # DB ì €ì¥ (UPSERT)
                    report_dict = canonical_report.model_dump(mode='json')
                    report_create = DailyReportCreate(
                        owner=canonical_report.owner,
                        report_date=canonical_report.period_start,
                        report_json=report_dict
                    )
                    
                    db_report, is_created = DailyReportRepository.create_or_update(
                        db, report_create
                    )
                    
                    total_reports += 1
                    if is_created:
                        created_count += 1
                    else:
                        updated_count += 1
                
                except Exception as e:
                    error_count += 1
                    continue
            
            # ì§„í–‰ë¥  í‘œì‹œ (10% ë‹¨ìœ„)
            if file_idx % max(1, total_files // 10) == 0 or file_idx == total_files:
                progress = int((file_idx / total_files) * 100)
                print(f"\r   ğŸ“„ ì§„í–‰ë¥ : {progress}% ({file_idx}/{total_files})", end="", flush=True)
        
        print()  # ì¤„ë°”ê¿ˆ
        
        # 5. ê²°ê³¼ ì¶œë ¥ (ê°„ëµí•˜ê²Œ)
        if error_count > 0:
            print(f"   ğŸ“Š ê²°ê³¼: ìƒì„± {created_count}ê°œ, ì—…ë°ì´íŠ¸ {updated_count}ê°œ, ì—ëŸ¬ {error_count}ê°œ")
        else:
            print(f"   ğŸ“Š ê²°ê³¼: ìƒì„± {created_count}ê°œ, ì—…ë°ì´íŠ¸ {updated_count}ê°œ")
        
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        db.close()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ì¼ì¼ë³´ê³ ì„œ Bulk Ingestion ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--preview",
        action="store_true",
        help="ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ (DB ì €ì¥í•˜ì§€ ì•ŠìŒ)"
    )
    parser.add_argument(
        "--year",
        type=int,
        help="í•„í„°ë§í•  ì—°ë„ (ì˜ˆ: 2025)"
    )
    parser.add_argument(
        "--month",
        type=int,
        help="í•„í„°ë§í•  ì›” (ì˜ˆ: 11)"
    )
    parser.add_argument(
        "--owner",
        type=str,
        help="ingestion ì‹œ ì‚¬ìš©í•  owner (ë¯¸ì§€ì • ì‹œ REPORT_OWNER í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)"
    )
    
    args = parser.parse_args()
    
    if args.preview:
        # ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ
        preview_files(year=args.year, month=args.month)
    else:
        # ì‹¤ì œ ì €ì¥ ëª¨ë“œ
        bulk_ingest_daily_reports(year=args.year, month=args.month, owner=args.owner)

