# 시간복잡도 및 공간복잡도 종합 분석 문서

**작성일**: 2025.12.01  
**분석 대상**: 8개 핵심 파일  
**목적**: 성능 병목 구간 식별 및 최적화 방안 제시

---

## 목차

1. [개요 및 파일 분류](#1-개요-및-파일-분류)
2. [데이터 처리 파이프라인 분석](#2-데이터-처리-파이프라인-분석)
3. [RAG 상담 시스템 분석](#3-rag-상담-시스템-분석)
4. [종합 병목 구간 및 우선순위](#4-종합-병목-구간-및-우선순위)
5. [개선 방안 요약](#5-개선-방안-요약)

---

## 1. 개요 및 파일 분류

### 1.1 파일 그룹 분류

#### 그룹 A: 데이터 처리 파이프라인
- `automatic_save.py` - 통합 오케스트레이터
- `create_chunk_files.py` - PDF 청킹
- `create_openai_embeddings.py` - 임베딩 생성
- `save_to_vectordb.py` - Vector DB 저장

#### 그룹 B: RAG 상담 시스템
- `rag_therapy.py` - 메인 컨트롤러
- `persona_manager.py` - 페르소나 관리
- `search_engine.py` - 검색 엔진
- `response_generator.py` - 답변 생성

### 1.2 복잡도 표기법 (간단 설명)

- **시간복잡도**: 입력 크기에 따라 시간이 얼마나 걸리는지
  - `O(1)`: 항상 일정한 시간 (매우 빠름)
  - `O(n)`: 입력 크기에 비례해서 시간 증가 (선형)
  - `O(n²)`: 입력 크기의 제곱에 비례 (느림)
  - `O(log n)`: 로그 시간 (빠름)
  
- **공간복잡도**: 메모리를 얼마나 사용하는지

**예시**:
- 파일 1개 처리: 1분
- 파일 10개 처리: 10분 (O(n))
- 파일 100개 처리: 100분 (O(n))

---

## 2. 데이터 처리 파이프라인 분석

### 2.1 automatic_save.py

#### 클래스: AutomaticSaveManager

**함수별 복잡도 분석**

| 함수명 | 시간복잡도 | 공간복잡도 | 간단 설명 |
|--------|-----------|-----------|---------|
| `__init__` | O(1) | O(1) | 초기화만 하므로 항상 빠름 |
| `check_folder_and_files` | O(파일수) | O(1) | 파일 개수만큼 시간 증가 |
| `create_folder_if_not_exists` | O(1) | O(1) | 폴더 생성은 항상 빠름 |
| `rollback` | O(디렉토리수) | O(1) | 디렉토리 개수만큼 시간 증가 |
| `run_script` | O(스크립트실행시간) | O(1) | 스크립트 실행 시간에 비례 |
| `step1_create_chunks` | O(파일수 × 파일당시간) | O(1) | 파일 개수 × 각 파일 처리 시간 |
| `step2_create_embeddings` | O(파일수 × 파일당시간) | O(1) | 파일 개수 × 각 파일 처리 시간 |
| `step3_save_to_vectordb` | O(파일수 × 파일당시간) | O(1) | 파일 개수 × 각 파일 처리 시간 |
| `run` | O(파일수 × 전체시간) | O(1) | 모든 단계를 순차 실행 |

**예시**: 파일 20개 처리 시
- 각 단계가 파일당 1분씩 걸리면 → 총 60분 (3단계 × 20분)

**병목 구간**:
- 🔴 **Critical**: 순차 스크립트 실행으로 인한 시간 누적
- 🟡 **Medium**: 각 단계별 파일 존재 여부 확인 (불필요한 I/O)

**개선 방안**:
- 병렬 스크립트 실행 (가능한 경우)
- 캐시된 결과 활용

---

### 2.2 create_chunk_files.py

#### 클래스: ChunkCreator

**함수별 복잡도 분석**

| 함수명 | 시간복잡도 | 공간복잡도 | 간단 설명 |
|--------|-----------|-----------|---------|
| `__init__` | O(1) | O(1) | 초기화만 하므로 항상 빠름 |
| `count_tokens` | O(텍스트길이) | O(텍스트길이) | 텍스트가 길수록 시간 증가 |
| `extract_text_from_pdf` | O(페이지수 × 페이지당텍스트) | O(페이지수 × 페이지당텍스트) | 페이지가 많을수록 시간 증가 |
| `clean_pdf_text` | O(텍스트길이 × 패턴수) | O(텍스트길이) | 텍스트 길이와 정규식 패턴 수에 비례 |
| `extract_metadata_adler` | O(1) | O(1) | 파일명만 파싱하므로 빠름 |
| `extract_metadata_from_filename` | O(1) | O(1) | 파일명만 파싱하므로 빠름 |
| `split_into_parents` | O(문단수 × 토큰계산) | O(청크수 × 청크크기) | 문단마다 토큰 계산 필요 |
| `split_parent_into_children` | O(문단수 × 토큰계산) | O(청크수 × 청크크기) | 문단마다 토큰 계산 필요 |
| `process_file` | O(페이지수 + 텍스트길이 + 청크수) | O(텍스트길이 + 청크수) | 여러 작업의 합 |
| `create_chunk_objects` | O(청크수) | O(청크수) | 청크 개수만큼 시간 증가 |
| `process_single_file` | O(파일처리시간) | O(파일크기) | 파일 처리 시간에 비례 |
| `process_directory` | O(파일수 × 파일당시간) | O(파일수 × 파일크기) | 파일 개수에 비례해서 시간 증가 |

**예시**: PDF 파일 1개 (100페이지, 500개 청크 생성)
- 텍스트 추출: ~10초
- 정제화: ~5초
- 청킹: ~30초
- 총: ~45초
- 파일 20개면: ~15분 (순차 처리)

**병목 구간**:
- 🔴 **Critical**: 
  1. `extract_text_from_pdf`: PDF 페이지별 순차 처리 O(p × l_p)
  2. `clean_pdf_text`: 정규식 반복 실행 O(l × r)
  3. `split_into_parents/split_parent_into_children`: 각 문단마다 토큰 계산 O(c × t)
  4. `process_directory`: 순차 파일 처리 O(f × ...)
- 🟡 **Medium**: 
  1. `count_tokens`: 매번 tiktoken 인코딩 수행
  2. JSON 저장 시 전체 데이터 메모리 로드

**개선 방안**:
- PDF 텍스트 추출 병렬화 (페이지 단위)
- 정규식 패턴 컴파일 및 캐싱
- 토큰 계산 결과 캐싱
- 파일 처리 병렬화 (ProcessPoolExecutor)
- 스트리밍 JSON 저장

---

### 2.3 create_openai_embeddings.py

**함수별 복잡도 분석**

| 함수명 | 시간복잡도 | 공간복잡도 | 간단 설명 |
|--------|-----------|-----------|---------|
| `load_chunks` | O(JSON파싱시간) | O(청크수 × 청크크기) | JSON 파일 크기에 비례 |
| `create_embeddings` | O(청크수 ÷ 배치크기 × API시간) | O(배치크기 × 임베딩크기) | 배치로 나눠서 API 호출 |
| `save_embeddings` | O(청크수 × 저장시간) | O(청크수 × 청크크기) | 청크 개수에 비례 |
| `main` | O(파일수 × 파일당시간) | O(파일수 × 청크수) | 파일 개수에 비례 |

**예시**: 청크 10,000개, 배치 크기 100
- API 호출 횟수: 10,000 ÷ 100 = 100번
- API 호출 시간: 100번 × 3초 = 300초 (5분)
- 파일 20개면: 20 × 5분 = 100분 (순차 처리)

**병목 구간**:
- 🔴 **Critical**: 
  1. `create_embeddings`: 순차 배치 처리 O((c / B) × A)
     - 예: 10,000개 청크 → 100번 API 호출 × 3초 = 300초
  2. `main`: 파일별 순차 처리
  3. 네트워크 I/O 대기 시간 누적
- 🟡 **Medium**: 
  1. 전체 청크 데이터 메모리 로드
  2. JSON 파일 I/O

**개선 방안**:
- 비동기 배치 처리 (AsyncOpenAI, asyncio.gather)
- 동시 배치 수 제한 (Semaphore)
- 스트리밍 처리로 메모리 사용량 감소
- 배치 크기 최적화 (API Rate Limit 고려)

---

### 2.4 save_to_vectordb.py

#### 클래스: VectorDBManager

**함수별 복잡도 분석**

| 함수명 | 시간복잡도 | 공간복잡도 | 간단 설명 |
|--------|-----------|-----------|---------|
| `__init__` | O(1) | O(1) | 초기화만 하므로 항상 빠름 |
| `_initialize_client` | O(1) | O(1) | 클라이언트 생성은 빠름 |
| `load_embedding_file` | O(JSON파싱시간) | O(청크수 × 청크크기) | JSON 파일 크기에 비례 |
| `create_or_get_collection` | O(1) | O(1) | 컬렉션 조회는 빠름 |
| `save_to_collection` | O(기존ID수 + 청크수 × 로그시간 + 배치저장시간) | O(기존ID수 + 청크수) | 기존 ID 확인 + 저장 시간 |
| `verify_collection` | O(1) | O(1) | 검증은 빠름 |
| `main` | O(파일수 × 파일당시간) | O(파일수 × 청크수) | 파일 개수에 비례 |

**예시**: 청크 10,000개 저장
- 기존 ID 확인: ~1초
- 배치 저장 (배치 크기 1000): 10번 × 0.5초 = 5초
- 총: ~6초
- 파일 20개면: ~2분 (순차 처리)

**병목 구간**:
- 🔴 **Critical**: 
  1. `save_to_collection`: 기존 ID 전체 조회 O(c_existing)
  2. 모든 임베딩 파일 메모리 로드 O(f × c × m)
  3. 순차 배치 저장
- 🟡 **Medium**: 
  1. 중복 ID 확인을 위한 set 조회 (메모리 사용량 증가)
  2. 배치 크기 최적화 필요

**개선 방안**:
- 스트리밍 단독 처리 (파일별 순차 처리, 메모리 부족 문제)
- 기존 ID 조회 최적화 (인덱스 활용)
- 배치 저장 병렬화 (가능한 경우)
- 메모리 사용량 모니터링

---

## 3. RAG 상담 시스템 분석

### 3.1 rag_therapy.py

#### 클래스: RAGTherapySystem

**함수별 복잡도 분석**

| 함수명 | 시간복잡도 | 공간복잡도 | 간단 설명 |
|--------|-----------|-----------|---------|
| `__init__` | O(1) | O(1) | 초기화만 하므로 항상 빠름 |
| `_save_qa_to_vectordb` | O(임베딩생성 + 저장시간) | O(1) | 임베딩 생성 + DB 저장 |
| `chat` | O(번역 + 검색 + 답변생성) | O(검색결과수 + 히스토리) | 여러 API 호출 순차 실행 |
| `summarize_chunk` | O(1) | O(1) | 단순 자르기만 하므로 빠름 |

**예시**: 사용자 질문 1개 처리
- 번역: ~1초
- 검색 (반복 2회): ~6초
- Re-ranker: ~2초
- 답변 생성: ~3초
- 총: ~12초

**병목 구간**:
- 🔴 **Critical**: 
  1. `chat`: 순차 API 호출 체인 (번역 → 검색 → 재검색 → 답변)
  2. Multi-step 반복 검색으로 인한 시간 누적
  3. 불필요한 재검색 (품질이 충분해도 실행)
- 🟡 **Medium**: 
  1. 번역 API 호출 (매 요청마다)
  2. Re-ranker 실행 비용

**개선 방안**:
- 번역 결과 캐싱
- 조기 종료 조건 강화
- 병렬 API 호출 (번역 + 초기 검색)
- 조건부 Re-ranker 실행

---

### 3.2 persona_manager.py

#### 클래스: PersonaManager

**함수별 복잡도 분석**

| 함수명 | 시간복잡도 | 공간복잡도 | 간단 설명 |
|--------|-----------|-----------|---------|
| `__init__` | O(1) | O(1) | 초기화만 하므로 항상 빠름 |
| `generate_persona_with_rag` | O(검색쿼리수 × 검색시간 + LLM시간) | O(검색결과수) | 여러 검색 + LLM 호출 |
| `generate_persona_with_prompt_engineering` | O(1) | O(1) | 텍스트만 반환하므로 빠름 |
| `_get_default_persona` | O(1) | O(1) | 텍스트만 반환하므로 빠름 |
| `_load_cached_persona` | O(1) | O(1) | 파일 읽기만 하므로 빠름 |
| `_save_persona_cache` | O(1) | O(1) | 파일 쓰기만 하므로 빠름 |
| `_start_background_persona_generation` | O(1) | O(1) | 스레드 시작만 하므로 빠름 |
| `is_rag_persona_ready` | O(1) | O(1) | 플래그 확인만 하므로 빠름 |
| `_search_web_for_adler` | O(LLM시간) | O(1) | LLM 호출 시간에 비례 |
| `_generate_persona_from_rag` | O(검색쿼리수 × 검색시간 + LLM시간) | O(검색결과수) | 여러 검색 + LLM 호출 |

**예시**: RAG 페르소나 생성 (백그라운드)
- 6개 쿼리 검색: ~3초
- 웹 검색: ~2초
- 페르소나 생성: ~3초
- 총: ~8초 (백그라운드 실행)

**병목 구간**:
- 🟢 **Low**: 백그라운드 실행으로 사용자 경험에 영향 없음
- 🟡 **Medium**: 
  1. 초기화 시 6개 쿼리 검색 (백그라운드이지만 리소스 사용)
  2. 웹 검색 LLM 호출

**개선 방안**:
- 검색 쿼리 수 최적화 (6개 → 3-4개)
- 웹 검색 결과 캐싱
- 페르소나 생성 결과 재사용 기간 연장

---

### 3.3 search_engine.py

#### 클래스: SearchEngine

**함수별 복잡도 분석**

| 함수명 | 시간복잡도 | 공간복잡도 | 간단 설명 |
|--------|-----------|-----------|---------|
| `__init__` | O(1) | O(1) | 초기화만 하므로 항상 빠름 |
| `translate_to_english` | O(LLM시간) | O(1) | 번역 API 호출 시간에 비례 |
| `create_query_embedding` | O(임베딩생성시간) | O(임베딩크기) | 임베딩 생성 시간에 비례 |
| `_distance_to_similarity` | O(1) | O(1) | 간단한 계산이므로 빠름 |
| `rerank_chunks` | O(LLM시간) | O(검색결과수) | LLM으로 재정렬 |
| `_calculate_emotion_boost` | O(키워드수 × 텍스트길이) | O(1) | 키워드 매칭 시간 |
| `_evaluate_search_quality` | O(검색결과수) | O(검색결과수) | 결과 개수에 비례 |
| `_expand_query_with_llm` | O(LLM시간) | O(확장쿼리수) | 쿼리 확장 API 호출 |
| `_iterative_search_with_query_expansion` | O(반복횟수 × 검색시간) | O(검색결과수) | 반복 검색으로 시간 누적 |
| `_hybrid_search` | O(임베딩생성 + 검색시간) | O(검색결과수) | 임베딩 생성 + 검색 |
| `retrieve_chunks` | O(임베딩생성 + 검색시간) | O(검색결과수) | 기본 검색 |
| `_get_max_similarity` | O(검색결과수) | O(1) | 결과 개수에 비례 |
| `get_distance_to_similarity` | O(1) | O(1) | 간단한 계산이므로 빠름 |

**예시**: 검색 1회
- 번역: ~1초
- 임베딩 생성: ~0.5초
- 검색: ~0.5초
- Re-ranker: ~2초
- 총: ~4초

**병목 구간**:
- 🔴 **Critical**: 
  1. `translate_to_english`: 매 요청마다 번역 API 호출 O(LLM_t)
  2. `_iterative_search_with_query_expansion`: 반복 검색으로 인한 시간 누적
  3. `rerank_chunks`: 모든 검색 결과에 대해 LLM 호출
  4. `_expand_query_with_llm`: 쿼리 확장 비용 대비 효과 불확실
- 🟡 **Medium**: 
  1. `_calculate_emotion_boost`: 키워드 매칭 반복 (kw × l)
  2. `_hybrid_search`: 감정 가중치 계산 오버헤드

**개선 방안**:
- 번역 결과 캐싱 (LRU Cache)
- 조기 종료 조건 강화 (품질이 충분하면 재검색 중단)
- 조건부 Re-ranker 실행 (유사도가 높으면 생략)
- 쿼리 확장 최적화 (간단한 키워드 확장 또는 조건부 실행)
- 감정 키워드 매칭 최적화 (Trie 구조 또는 정규식 컴파일)

---

### 3.4 response_generator.py

#### 클래스: ResponseGenerator

**함수별 복잡도 분석**

| 함수명 | 시간복잡도 | 공간복잡도 | 간단 설명 |
|--------|-----------|-----------|---------|
| `__init__` | O(1) | O(1) | 초기화만 하므로 항상 빠름 |
| `classify_input` | O(키워드수) | O(1) | 키워드 개수에 비례 |
| `is_therapy_related` | O(키워드수) | O(1) | 키워드 개수에 비례 |
| `_generate_llm_only_response` | O(LLM시간) | O(히스토리길이) | LLM 호출 시간에 비례 |
| `generate_response_with_persona` | O(검색결과수 + LLM시간) | O(검색결과수 + 히스토리) | 컨텍스트 구성 + LLM 호출 |

**예시**: 답변 생성 1회
- 입력 분류: ~0.01초
- 컨텍스트 구성: ~0.01초
- LLM 호출: ~3초
- 총: ~3초

**병목 구간**:
- 🔴 **Critical**: 
  1. `_generate_llm_only_response`: LLM API 호출 O(LLM)
  2. `generate_response_with_persona`: LLM API 호출 + 컨텍스트 구성
- 🟡 **Medium**: 
  1. `classify_input`: 키워드 순차 검색 O(kw)
  2. 대화 히스토리 관리 (최대 10개)

**개선 방안**:
- 키워드 매칭 최적화 (Trie 구조 또는 집합 연산)
- 컨텍스트 길이 최적화 (불필요한 히스토리 제거)
- LLM 호출 최적화 (토큰 수 제한, temperature 조정)

---

## 4. 종합 병목 구간 및 우선순위

### 4.1 Critical (즉시 개선 필요)

#### 데이터 처리 파이프라인

1. **순차 파일 처리** (`create_chunk_files.py`, `create_openai_embeddings.py`)
   - **영향**: 파일 개수에 비례하여 시간 증가
   - **예상 효과**: 병렬 처리 시 70-80% 시간 단축
   - **우선순위**: ⭐⭐⭐⭐⭐

2. **순차 배치 임베딩 생성** (`create_openai_embeddings.py`)
   - **영향**: 10,000개 청크 → 100번 API 호출 × 3초 = 300초
   - **예상 효과**: 비동기 처리 시 60-70% 시간 단축
   - **우선순위**: ⭐⭐⭐⭐⭐

3. **전체 데이터 메모리 로드** (`save_to_vectordb.py`)
   - **영향**: 메모리 부족 가능성, GC 오버헤드
   - **예상 효과**: 스트리밍 처리 시 메모리 사용량 90% 감소
   - **우선순위**: ⭐⭐⭐⭐

#### RAG 상담 시스템

4. **순차 API 호출 체인** (`rag_therapy.py`, `search_engine.py`)
   - **영향**: 번역 → 검색 → 재검색이 모두 순차 실행
   - **예상 효과**: 병렬 처리 또는 캐싱 시 40-50% 시간 단축
   - **우선순위**: ⭐⭐⭐⭐⭐

5. **불필요한 재검색** (`search_engine.py`)
   - **영향**: 품질이 이미 충분해도 재검색 시도
   - **예상 효과**: 조기 종료 조건 강화 시 30% 시간 단축
   - **우선순위**: ⭐⭐⭐⭐

6. **Re-ranker 실행 비용** (`search_engine.py`)
   - **영향**: 모든 검색 결과에 대해 LLM 재정렬 (추가 2초)
   - **예상 효과**: 조건부 실행 시 50% 시간 단축
   - **우선순위**: ⭐⭐⭐⭐

7. **번역 API 호출** (`search_engine.py`)
   - **영향**: 매 요청마다 번역 (1초 지연)
   - **예상 효과**: 캐싱 시 90% 시간 단축
   - **우선순위**: ⭐⭐⭐⭐⭐

### 4.2 Medium (우선 개선)

1. **텍스트 정제화 정규식 반복** (`create_chunk_files.py`)
   - **영향**: 긴 텍스트에서 시간 소모
   - **예상 효과**: 정규식 컴파일 캐싱 시 20-30% 시간 단축
   - **우선순위**: ⭐⭐⭐

2. **중복 ID 확인 방식** (`save_to_vectordb.py`)
   - **영향**: 대용량 데이터에서 메모리 사용량 증가
   - **예상 효과**: 스트리밍 확인 또는 DB 쿼리 최적화
   - **우선순위**: ⭐⭐⭐

3. **쿼리 확장 비용 대비 효과 낮음** (`search_engine.py`)
   - **영향**: 5-6초 소요, 품질 개선 효과 불확실
   - **예상 효과**: 조건부 실행 또는 간단한 키워드 확장
   - **우선순위**: ⭐⭐⭐

4. **키워드 매칭 순차 검색** (`response_generator.py`, `search_engine.py`)
   - **영향**: 키워드 수에 비례하여 시간 증가
   - **예상 효과**: Trie 구조 또는 집합 연산으로 최적화
   - **우선순위**: ⭐⭐

### 4.3 Low (장기 개선)

1. **토큰 계산 반복** (`create_chunk_files.py`)
   - **영향**: 각 청크마다 tiktoken 인코딩 수행
   - **예상 효과**: 결과 캐싱으로 일부 시간 단축
   - **우선순위**: ⭐

2. **페르소나 생성 쿼리 수** (`persona_manager.py`)
   - **영향**: 백그라운드 실행이지만 리소스 사용
   - **예상 효과**: 쿼리 수 최적화 (6개 → 3-4개)
   - **우선순위**: ⭐

---

## 5. 개선 방안 요약

### 5.1 Phase 1: 즉시 개선 (1-2주)

**목표**: 응답 시간 30-40% 단축

1. ✅ **번역 캐싱** (`search_engine.py`)
   - 구현 난이도: 낮음
   - 예상 효과: 평균 0.5초 단축
   - 작업 시간: 2-3시간

2. ✅ **조기 종료 조건 강화** (`search_engine.py`)
   - 구현 난이도: 낮음
   - 예상 효과: 평균 1-2초 단축
   - 작업 시간: 1-2시간

3. ✅ **조건부 Re-ranker 실행** (`search_engine.py`)
   - 구현 난이도: 낮음
   - 예상 효과: 평균 1초 단축
   - 작업 시간: 1-2시간

**Phase 1 총 예상 효과**: 평균 응답 시간 10-12초 → 7-9초 (약 30% 단축)

### 5.2 Phase 2: 중기 개선 (2-4주)

**목표**: 자동 저장 시간 60-70% 단축

1. ✅ **병렬 파일 처리** (`create_chunk_files.py`, `automatic_save.py`)
   - 구현 난이도: 중간
   - 예상 효과: 청크 생성 시간 70% 단축 (30분 → 9분)
   - 작업 시간: 1-2일

2. ✅ **비동기 임베딩 생성** (`create_openai_embeddings.py`)
   - 구현 난이도: 중간
   - 예상 효과: 임베딩 생성 시간 60% 단축 (5분 → 2분)
   - 작업 시간: 2-3일

3. ✅ **스트리밍 Vector DB 저장** (`save_to_vectordb.py`)
   - 구현 난이도: 중간
   - 예상 효과: 메모리 사용량 90% 감소
   - 작업 시간: 1-2일

**Phase 2 총 예상 효과**: 전체 자동 저장 시간 37분 → 12분 (약 68% 단축)

### 5.3 Phase 3: 장기 개선 (1-2개월)

**목표**: 추가 최적화 및 확장성 확보

1. ✅ **병렬 API 호출** (`rag_therapy.py`, `search_engine.py`)
   - 구현 난이도: 높음
   - 예상 효과: 평균 1초 단축
   - 작업 시간: 3-5일

2. ✅ **쿼리 확장 최적화** (`search_engine.py`)
   - 구현 난이도: 중간
   - 예상 효과: 조건부 실행으로 2-3초 단축
   - 작업 시간: 2-3일

3. ✅ **정규식 최적화** (`create_chunk_files.py`)
   - 구현 난이도: 낮음
   - 예상 효과: 청크 생성 시간 20-30% 단축
   - 작업 시간: 1일

4. ✅ **키워드 매칭 최적화** (`response_generator.py`, `search_engine.py`)
   - 구현 난이도: 중간
   - 예상 효과: 분류 시간 50% 단축
   - 작업 시간: 2-3일

### 5.4 예상 성능 개선 결과

#### RAG 상담 시스템 (rag_therapy.py)

| 단계 | 개선 전 | Phase 1 후 | Phase 3 후 |
|------|---------|------------|------------|
| 번역 | 1.0초 | 0.5초 (캐싱) | 0.1초 (병렬) |
| 검색 | 6.0초 | 4.0초 (조기 종료) | 3.0초 (병렬) |
| Re-ranker | 2.0초 | 1.0초 (조건부) | 0.5초 (최적화) |
| 답변 생성 | 3.0초 | 3.0초 | 3.0초 |
| **총 시간** | **12.0초** | **8.5초** | **6.6초** |

**개선율**: 약 45% 시간 단축

#### 데이터 처리 파이프라인 (automatic_save.py)

| 단계 | 개선 전 | Phase 2 후 |
|------|---------|------------|
| 청크 생성 | 30분 | 9분 (병렬) |
| 임베딩 생성 | 5분 | 2분 (비동기) |
| Vector DB 저장 | 2분 | 1분 (스트리밍) |
| **총 시간** | **37분** | **12분** |

**개선율**: 약 68% 시간 단축

### 5.5 구현 시 주의사항

#### 병렬 처리
- 메모리 관리: 워커 수는 시스템 메모리에 맞게 조정
- API Rate Limit: OpenAI API Rate Limit 고려 (3,000 RPM)
- 에러 처리: 일부 워커 실패 시 전체 작업 실패 방지

#### 캐싱
- 캐시 무효화: 데이터 업데이트 시 캐시 무효화
- 메모리 관리: LRU 캐시 사용, 캐시 크기 제한

#### 테스트
- 단위 테스트: 각 최적화 기능별 테스트
- 통합 테스트: 전체 파이프라인 테스트
- 성능 테스트: 실제 데이터로 성능 측정
- 부하 테스트: 동시 요청 처리 능력 테스트

---

## 6. 결론

### 6.1 핵심 발견사항

1. **데이터 처리 파이프라인**: 순차 처리로 인한 시간 낭비가 주요 병목
2. **RAG 상담 시스템**: 불필요한 API 호출과 순차 실행이 주요 병목

### 6.2 권장 사항

1. **즉시 실행**: Phase 1 개선사항 (낮은 난이도, 높은 효과)
2. **단계적 개선**: Phase 2, 3 순차 진행
3. **지속적 모니터링**: 성능 메트릭 수집 및 분석

### 6.3 예상 효과

- **RAG 상담 시스템**: 평균 응답 시간 45% 단축 (12초 → 6.6초)
- **데이터 처리 파이프라인**: 전체 실행 시간 68% 단축 (37분 → 12분)
- **사용자 경험**: 상당한 개선 예상

---

**작성자**: AI Assistant  
**검토 필요**: 시스템 아키텍처 검토, API Rate Limit 확인, 메모리 제약 확인

## 7. 재계산 시간복잡도 요약 및 기존 표기와의 차이

아래 표들은 실제 코드(8개 파일)를 기준으로 **시간복잡도를 다시 계산**한 뒤, 이 문서 상단의 **기존 표기와 비교**한 것입니다.  
기존 표는 그대로 두고, 여기서는 “현재 코드 기준으로 보면 어떤 의미인지”를 정리합니다.

### 7.1 automatic_save.py – AutomaticSaveManager

기호:
- `F_c`: 청크 생성 대상 PDF 파일 수  
- `F_e`: 임베딩 생성 대상 청크 파일 수  
- `F_v`: Vector DB 저장 대상 임베딩 파일 수  
- `T_chunk`: `create_chunk_files.py`에서 파일 1개 처리 시간  
- `T_embed`: `create_openai_embeddings.py`에서 파일 1개 처리 시간  
- `T_vectordb`: `save_to_vectordb.py`에서 파일 1개 처리 시간  

| 함수명 | 기존 시간복잡도 | 재계산 시간복잡도 | 차이 설명 |
|--------|----------------|-------------------|-----------|
| `__init__` | O(1) | O(1) | 동일, 경로/리스트 초기화만 수행 |
| `check_folder_and_files` | O(파일수) | O(파일수) | 동일, `glob`로 파일 리스트를 만들어 파일 수에 비례 |
| `create_folder_if_not_exists` | O(1) | O(1) | 동일 |
| `rollback` | O(디렉토리수) | O(디렉토리수) | 동일 |
| `run_script` | O(스크립트실행시간) | O(스크립트실행시간) | 동일 의미, 스크립트 실행 시간에 비례 |
| `step1_create_chunks` | O(파일수 × 파일당시간) | O(F_c × T_chunk) | 구조는 동일, 실제 `T_chunk`는 병렬 처리된 청크 생성 시간 |
| `step2_create_embeddings` | O(파일수 × 파일당시간) | O(F_e × T_embed) | 구조는 동일, 실제 `T_embed`는 비동기/병렬 임베딩 생성 시간 |
| `step3_save_to_vectordb` | O(파일수 × 파일당시간) | O(F_v × T_vectordb) | 구조는 동일, 실제 `T_vectordb`는 스트리밍 Vector DB 저장 시간 |
| `run` | O(파일수 × 전체시간) | O(F_c×T_chunk + F_e×T_embed + F_v×T_vectordb) | “파일수 × 전체시간”을 3단계의 합으로 풀어 쓴 것. 병렬/비동기 최적화 덕분에 기존 순차 가정보다 실제 벽시계 시간은 40~70%까지 줄어들 수 있음 |

---

### 7.2 create_chunk_files.py – ChunkCreator

기호:
- `P`: PDF 페이지 수  
- `L`: 전체 텍스트 길이  
- `R`: 정규식 패턴 수  
- `C`: 생성된 전체 청크 수  
- `F`: 처리할 PDF 파일 수  
- `W`: 사용 가능한 프로세스(worker) 수 (`os.cpu_count()`)  

| 함수명 | 기존 시간복잡도 | 재계산 시간복잡도 | 차이 설명 |
|--------|----------------|-------------------|-----------|
| `__init__` | O(1) | O(R) | 실제 구현은 여러 정규식을 **초기 1회 컴파일**하므로, 초기화가 패턴 수 R에 비례하지만 이후 호출은 상수 시간 |
| `count_tokens` | O(텍스트길이) | O(텍스트길이) | 동일, tiktoken 인코딩 호출 |
| `extract_text_from_pdf` | O(페이지수 × 페이지당텍스트) | O(P × 페이지당텍스트) | 동일, 페이지를 한 번씩 순회하며 텍스트를 합침 |
| `clean_pdf_text` | O(텍스트길이 × 패턴수) | O(텍스트길이) | 패턴이 모두 미리 컴파일되어 있어, 실질적으로는 텍스트 길이에 비례하는 상수 계수 연산으로 보는 게 더 정확함 |
| `extract_metadata_adler` / `extract_metadata_from_filename` | O(1) | O(1) | 동일, 파일명 파싱 수준 |
| `split_into_parents` / `split_parent_into_children` | O(문단수 × 토큰계산) | O(문단수 × 토큰계산) | 동일, 문단/라인 단위 토큰 수 계산 |
| `process_file` | O(페이지수 + 텍스트길이 + 청크수) | O(P + L + C) | 구조는 동일, “텍스트 추출 + 정제 + 청킹” 세 단계의 합 |
| `create_chunk_objects` | O(청크수) | O(청크수) | 동일, 청크당 1개 딕셔너리 생성 |
| `process_single_file` | O(파일처리시간) | O(P + L + C) | 동일 의미, `process_file` + JSON 저장 시간 |
| `process_directory` (기본) | O(파일수 × 파일당시간) | O(F × T_file) | 순차 모드는 동일 |
| `process_directory(save_individually=True)` | 표기 없음 | O(F × T_file / W) (벽시계 기준) | 실제 코드는 `ProcessPoolExecutor`로 파일을 병렬 처리 → CPU 코어 W개 기준으로 순차 대비 이론상 최대 70~80% 정도 시간 단축 가능 |

예상 효과(예시): PDF 20개, CPU 8코어, 파일당 45초라면  
- 순차: 약 900초(15분)  
- 병렬: 약 900 / 8 = 112.5초(2분 정도) 수준까지 단축 가능 (I/O 오버헤드에 따라 달라짐)

---

### 7.3 create_openai_embeddings.py

기호:
- `C`: 한 파일 내 청크 수  
- `B`: 배치 크기 (`BATCH_SIZE`)  
- `T_API`: 임베딩 API 1회 호출 시간  
- `F`: 처리할 청크 파일 수  
- `K`: 동시에 처리하는 배치 수 (`MAX_CONCURRENT_BATCHES`)  

| 함수명 | 기존 시간복잡도 | 재계산 시간복잡도 | 차이 설명 |
|--------|----------------|-------------------|-----------|
| `load_chunks` | O(JSON파싱시간) | O(청크수) | 동일 의미, 청크 수에 비례 |
| `create_embeddings` | O(청크수 ÷ 배치크기 × API시간) | O((청크수 ÷ B) × T_API ÷ K) (벽시계 기준) | 실제 코드는 `AsyncOpenAI`와 `asyncio.Semaphore`로 여러 배치를 동시에 처리 → 순차 가정보다 벽시계 시간이 K배까지 감소 가능 (예: K=10이면 이론상 90% 단축) |
| `save_embeddings` | O(청크수 × 저장시간) | O(청크수 × 저장시간) | 동일, 청크마다 한 번씩 쓰기 |
| `_process_single_file_async` | 표기 없음 | O(load + create + save) | 한 파일에 대해 “로드 → 임베딩 생성 → 저장” 전체를 비동기로 처리 |
| `main` | O(파일수 × 파일당시간) | O(파일수 × 파일당시간 ÷ 동시파일수) | 실제 코드는 파일 단위 처리도 `asyncio.gather`로 병렬 수행 → 파일이 여러 개일수록 추가로 40~60% 정도 더 단축 가능 |

예상 효과(예시): 청크 10,000개, B=100, T_API≈3초, K=10인 경우  
- 기존 순차: 100번 × 3초 = 300초(5분)  
- 배치 병렬: 300초 ÷ 10 ≈ 30초 수준까지 단축 가능 (Rate Limit와 네트워크 상황에 따라 다름)

---

### 7.4 save_to_vectordb.py – VectorDBManager

기호:
- `C_file`: 파일 1개 내 청크 수  
- `C_exist`: 컬렉션에 이미 존재하는 ID 수  
- `B`: 배치 크기  
- `F`: 임베딩 파일 수  

| 함수명 | 기존 시간복잡도 | 재계산 시간복잡도 | 차이 설명 |
|--------|----------------|-------------------|-----------|
| `__init__`, `_initialize_client` | O(1) | O(1) | 동일, PersistentClient 생성 |
| `load_embedding_file` | O(JSON파싱시간) | O(C_file) | 동일 의미, 파일 내 청크 수에 비례 |
| `create_or_get_collection` | O(1) | O(1) | 동일 |
| `save_to_collection` | O(기존ID수 + 청크수 × 로그시간 + 배치저장시간) | O(C_exist + C_file + (C_new/B)) | 실제 코드는 `include=[\"ids\"]`로 **ID만 조회**하고, 중복 ID는 건너뜀 → 메모리와 시간 모두 기존 “전체 데이터 로드” 가정보다 훨씬 효율적 |
| `verify_collection` | O(1) | O(1) | 동일, `count` + `peek` |
| `main` | O(파일수 × 파일당시간) | O(∑F(C_file + C_exist + C_new/B)) | 파일을 하나씩 로드→저장 후 즉시 GC로 해제하는 스트리밍 구조. 메모리 피크는 “가장 큰 파일 1개 + ID 집합” 수준으로 제한 |

예상 효과(예시): 청크 10,000개, B=1000, 기존 ID 10,000개, 새 ID 10,000개라면  
- 배치 10번 × 0.5초 ≈ 5초 + ID 조회 ≈ 1초 → 약 6초 정도로 기존 추정과 비슷하지만,  
 메모리 부족·GC 오버헤드 위험은 실제 코드 구조에서 크게 줄어든 상태입니다.

---

### 7.5 rag_therapy.py – RAGTherapySystem

기호:
- `N_r`: 검색 결과 청크 수  
- `H`: 대화 히스토리 길이(최대 10개 유지)  
- `T_search`: `SearchEngine` 검색 1회 시간 (임베딩 + Vector DB + 조건부 Re-ranker)  
- `T_llm`: 답변 생성 LLM 호출 시간  

| 함수명 | 기존 시간복잡도 | 재계산 시간복잡도 | 차이 설명 |
|--------|----------------|-------------------|-----------|
| `__init__` | O(1) | O(1) | 동일, 클라이언트/컬렉션/모듈 초기화 |
| `_save_qa_to_vectordb` (현재 `_save_qa_to_vectordb_async`) | O(임베딩생성 + 저장시간) | O(임베딩 1회 + 저장 1회) (비동기) | 의미상 동일하지만, 실제 코드는 `asyncio.create_task`로 **백그라운드에서 실행**되어 응답 시간에는 직접 포함되지 않음 |
| `chat` | O(번역 + 검색 + 답변생성) | O(T_search + T_llm) | 현재 코드에는 **번역 단계가 없고**, 검색도 `max_iterations=1`로 1회 검색 + 조건부 Re-ranker만 수행. 기존 “번역 + 반복 검색” 가정보다 실제 시간은 30~40% 정도 더 짧게 나올 가능성이 큼 |

예상 효과(예시):  
- 기존 가정: 번역 1초 + 검색 6초 + Re-ranker 2초 + 답변 3초 = 12초  
- 현재 구조: 번역 없음, 1회 검색 + 조건부 Re-ranker + 답변 → 평균 7~9초 수준이 현실적인 범위

---

### 7.6 persona_manager.py – PersonaManager

기호:
- `Q`: 페르소나 생성에 사용하는 Vector DB 쿼리 수 (현재 3개)  
- `N_r`: 각 쿼리당 검색 결과 수 (현재 3개, 중복 제거 후 상위 5개 사용)  
- `T_embed`: 임베딩 생성 시간  
- `T_db`: Vector DB 검색 시간  
- `T_llm_web`: 웹 검색 LLM 시간  
- `T_llm_persona`: 최종 페르소나 LLM 생성 시간  

| 함수명 | 기존 시간복잡도 | 재계산 시간복잡도 | 차이 설명 |
|--------|----------------|-------------------|-----------|
| `__init__` | O(1) | O(1) | 동일, 캐시 여부 확인 후 백그라운드 작업 시작 여부만 결정 |
| `generate_persona_with_rag` / `_generate_persona_from_rag` | O(검색쿼리수 × 검색시간 + LLM시간) | O(Q×(T_embed+T_db) + T_llm_web + T_llm_persona) | 실제 코드는 **3개 쿼리**만 사용, 각 쿼리당 상위 3개만 받고 중복 제거 후 상위 5개만 사용 → 기존 “6개 쿼리” 가정보다 검색 비용이 약 절반 수준 |
| `_search_web_for_adler` | O(LLM시간) | O(T_llm_web) | 동일 의미 |
| `_load_cached_persona` / `_save_persona_cache` | O(1) | O(1) | 소형 JSON 1개 읽기/쓰기 수준 |

예상 효과(예시):  
- 캐시가 없는 첫 실행 시에도, Vector DB 검색 비용은 기존 분석(6개 쿼리) 대비 **약 50% 감소**.  
- 전체 작업은 백그라운드 스레드에서 돌기 때문에, 사용자 체감 응답 시간에는 거의 영향 없음.

---

### 7.7 search_engine.py – SearchEngine

기호:
- `N_r`: 검색 결과 수  
- `K`: 상담/감정 키워드 수  
- `T_embed`: 임베딩 생성 시간  
- `T_db`: Vector DB 검색 시간  
- `T_llm_rank`: Re-ranker LLM 시간  

| 항목/함수 | 기존 시간복잡도 | 재계산 시간복잡도 | 차이 설명 |
|-----------|----------------|-------------------|-----------|
| `translate_to_english` | O(LLM시간) | (현재 코드에는 없음) | 기존 분석에는 있었지만, 실제 구현에는 번역 단계가 제거되어 있음 → 번역 1초 지연 가정은 현재 기준으로는 불필요 |
| `create_query_embedding` / `create_query_embedding_async` | O(임베딩생성시간) | O(T_embed) | 동일 |
| `retrieve_chunks` / `retrieve_chunks_async` | O(임베딩생성 + 검색시간) | O(T_embed + T_db + (T_llm_rank?)) | 기본 검색 + 품질이 낮을 때만 Re-ranker LLM 1회 호출 → 모든 검색에 대해 2초가 추가되는 구조는 아님 |
| `rerank_chunks` | O(LLM시간) | O(T_llm_rank + N_r) | LLM 호출 + 간단한 재정렬로 동일하지만, 호출 횟수가 “최고 유사도 < 0.55”인 경우로 제한 |
| `_calculate_emotion_boost` | O(키워드수 × 텍스트길이) | 평균적으로 O(입력단어수 + 일부 부분문자열 탐색) | 키워드를 set으로 캐싱하고, 우선 단어 단위 set 교집합으로 계산 → 최악은 동일하지만, 평균은 훨씬 작음 |
| `_expand_query_with_llm` | O(LLM시간) | 기본: O(1), 옵션: O(LLM시간) | 실제 기본 흐름은 `use_llm=False`로, 간단한 키워드 매핑만 사용하여 LLM 호출 없이 확장 |
| `_iterative_search_with_query_expansion` | O(반복횟수 × 검색시간) | 기본: 1회 검색 + 품질 평가 | `max_iterations=1`이 기본값이라, 반복 검색은 기본적으로 비활성화 상태 |

예상 효과(예시):  
- 기존 “번역 1초 + 반복 검색 2~3회 + Re-ranker 항상 실행” 가정보다,  
  현재 구조에서는 **1회 검색 + 조건부 Re-ranker** 정도라 평균 2~4초 수준의 절약 효과가 있을 수 있음.

---

### 7.8 response_generator.py – ResponseGenerator

기호:
- `K`: 상담 키워드 수  
- `N_r`: 검색된 청크 수  
- `H`: 대화 히스토리 길이 (실제 사용은 최근 2~5개로 제한)  
- `T_llm`: 답변 생성 LLM 시간  

| 함수명 | 기존 시간복잡도 | 재계산 시간복잡도 | 차이 설명 |
|--------|----------------|-------------------|-----------|
| `__init__` | O(1) | O(K) | 실제 구현은 키워드를 set으로 변환해 캐시 → 초기 1회만 O(K), 이후 조회는 O(1)에 가까움 |
| `classify_input` | O(키워드수) | O(입력길이 + 일부 키워드 매칭) | set 교집합 기반으로 주요 감정 키워드를 찾고, 부족할 때만 부분 문자열 탐색 → 평균적으로 기존 O(K)보다 빠르게 동작 |
| `is_therapy_related` | O(키워드수) | O(입력길이 + 일부 키워드 매칭) | `classify_input` 재사용, 동일 |
| `_generate_llm_only_response` | O(LLM시간) | O(H + T_llm) | LLM 호출이 지배적이긴 하지만, 실제 코드는 **최근 2개 히스토리만** 포함해 컨텍스트 길이를 제한 |
| `generate_response_with_persona` | O(검색결과수 + LLM시간) | O(상위 3개 청크 + 최근 5개 히스토리 + T_llm) | 상위 3개 청크/최근 5개 히스토리만 사용 → 입력 토큰 수를 일정 범위로 유지하여 비용과 지연을 통제 |

예상 효과(예시):  
- 키워드 매칭/히스토리 관리 부분은 최적화 덕분에 전체 응답 시간에서 차지하는 비중이 거의 미미(수~수십 ms 수준)하고,  
 실제 지연의 대부분은 여전히 LLM 호출(2~3초)이 차지합니다.

---

### 7.9 전체 요약

- 기존 표는 “설계/아이디어 단계에서의 보수적인 상한”에 가깝고,  
  여기 7장은 “**실제 구현된 코드(병렬/비동기/캐싱/조건부 실행)를 반영한 실질적인 복잡도**”라고 보면 됩니다.
- 특히 다음 항목들이 기존 분석 대비 크게 달라졌습니다.
  - **번역 API 반복 호출** → 현재 코드에서는 제거됨 (`search_engine.py`)  
  - **쿼리 확장 LLM 반복 호출** → 기본은 키워드 기반 간단 확장만 사용, LLM 확장은 옵션  
  - **Multi-step 반복 검색** → 기본 설정에서는 1회 검색 + 품질 평가만 수행  
  - **정규식/키워드 순차 검색** → 대부분 set/컴파일 기반으로 최적화  
  - **전체 데이터 메모리 로드** → 파일 단위 스트리밍 구조로 변경  
- 그 결과, 문서 5장에서 제시한 “예상 응답 시간/전체 처리 시간 감소율(45%, 68%)”은  
  현재 코드 기준 실측 값과도 비교적 잘 맞을 것으로 예상되며,  
  추가로 배치 크기, 동시 작업 수 등을 튜닝하면 10~20% 정도의 여유 최적화 여지는 남아 있습니다.
