"""
ì„ì‹œ RAG (Ephemeral RAG) ëª¨ë“ˆ - JSON ê¸°ë°˜

ì„¸ì…˜ë³„ ì„ì‹œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤:
1. Q3 ììœ ì—°ìƒ ì…ë ¥ì„ ì„ë² ë”©
2. JSON íŒŒì¼ì— ì €ì¥ (ëˆˆìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥!)
3. Q1 ëª©ì ê³¼ Q3 ì—°ìƒ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
4. ì˜êµ¬ RAG (SCAMPER, Mind Mapping, Starbursting)ì™€ ê²°í•©í•˜ì—¬ ì•„ì´ë””ì–´ ìƒì„±

ë³€ê²½ì‚¬í•­ (2024-11-30):
- ChromaDB â†’ JSON íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½
- ë°ì´í„°ê°€ ëˆˆì— ë³´ì´ê³  ë””ë²„ê¹…ì´ ì‰¬ì›Œì§
- ì„¸ì…˜ë³„ë¡œ data/ephemeral/{session_id}/associations.jsonì— ì €ì¥
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
from openai import OpenAI
from dotenv import load_dotenv
import os
import shutil


class EphemeralRAG:
    """
    ì„¸ì…˜ë³„ ì„ì‹œ RAG ì²˜ë¦¬ í´ë˜ìŠ¤ (JSON ê¸°ë°˜)
    
    ê° ì„¸ì…˜ë§ˆë‹¤ ë…ë¦½ì ì¸ JSON íŒŒì¼ì„ ìƒì„±í•˜ê³ ,
    Q3 ììœ ì—°ìƒ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, session_id: str):
        """
        ì´ˆê¸°í™”
        
        Args:
            session_id: ì„¸ì…˜ ID
        """
        # .env íŒŒì¼ ë¡œë“œ
        load_dotenv()
        
        self.session_id = session_id
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-large")
        
        # ì„¸ì…˜ë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        current_file = Path(__file__).resolve()
        module_dir = current_file.parent
        self.ephemeral_dir = module_dir / "data" / "ephemeral" / session_id
        self.json_path = self.ephemeral_dir / "associations.json"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.ephemeral_dir.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„° ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”
        self.data = self._load_data()
        
        print(f"âœ… Ephemeral RAG ì´ˆê¸°í™” (JSON ê¸°ë°˜)")
        print(f"   ğŸ“ ì €ì¥ ê²½ë¡œ: {self.json_path}")
    
    def _load_data(self) -> Dict:
        """JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        if self.json_path.exists():
            with open(self.json_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "session_id": self.session_id,
            "associations": []  # [{text, embedding}, ...]
        }
    
    def _save_data(self):
        """JSON íŒŒì¼ì— ë°ì´í„° ì €ì¥"""
        with open(self.json_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
    
    def embed_text(self, text: str) -> List[float]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[float]: ì„ë² ë”© ë²¡í„°
        """
        try:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            raise
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        a = np.array(vec1)
        b = np.array(vec2)
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))
    
    def add_associations(self, associations: List[str]) -> bool:
        """
        Q3 ììœ ì—°ìƒ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ì—¬ JSONì— ì¶”ê°€
        
        Args:
            associations: ììœ ì—°ìƒ ë‹¨ì–´/ë¬¸êµ¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            for association in associations:
                # ì„ë² ë”© ìƒì„±
                embedding = self.embed_text(association)
                
                self.data["associations"].append({
                    "text": association,
                    "embedding": embedding
                })
            
            # JSON íŒŒì¼ì— ì €ì¥
            self._save_data()
            
            print(f"âœ… {len(associations)}ê°œì˜ ì—°ìƒ ë‹¨ì–´ë¥¼ JSONì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            print(f"   ğŸ“„ íŒŒì¼: {self.json_path}")
            return True
            
        except Exception as e:
            print(f"âŒ ì—°ìƒ ë‹¨ì–´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def extract_keywords_by_similarity(self, purpose: str, top_k: int = 5) -> List[Dict]:
        """
        Q1 ëª©ì ê³¼ Q3 ì—°ìƒ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            purpose: Q1 ëª©ì  (ì˜ˆ: "ëª¨ë°”ì¼ ì•± ì•„ì´ë””ì–´")
            top_k: ì¶”ì¶œí•  ìƒìœ„ í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            List[Dict]: ìƒìœ„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ê°ê° keyword, similarity í¬í•¨)
        """
        try:
            if not self.data["associations"]:
                print("âš ï¸ ì €ì¥ëœ ì—°ìƒ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # Q1 ëª©ì  ì„ë² ë”©
            purpose_embedding = self.embed_text(purpose)
            
            # ëª¨ë“  ì—°ìƒ ë‹¨ì–´ì™€ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for item in self.data["associations"]:
                similarity = self._cosine_similarity(purpose_embedding, item["embedding"])
                similarities.append({
                    "keyword": item["text"],
                    "similarity": similarity
                })
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ kê°œ ì¶”ì¶œ
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            keywords = similarities[:top_k]
            
            print(f"\nâœ… Q1ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ìƒìœ„ {len(keywords)}ê°œ í‚¤ì›Œë“œ:")
            for i, kw in enumerate(keywords, 1):
                print(f"   {i}. {kw['keyword']} (ìœ ì‚¬ë„: {kw['similarity']:.4f})")
            
            return keywords
            
        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def search_associations(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        ì €ì¥ëœ ì—°ìƒ ë‹¨ì–´ì—ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            n_results: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            List[Dict]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not self.data["associations"]:
                return []
            
            query_embedding = self.embed_text(query)
            
            # ëª¨ë“  ì—°ìƒ ë‹¨ì–´ì™€ ìœ ì‚¬ë„ ê³„ì‚°
            results = []
            for item in self.data["associations"]:
                similarity = self._cosine_similarity(query_embedding, item["embedding"])
                results.append({
                    "document": item["text"],
                    "similarity": similarity
                })
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ nê°œ ë°˜í™˜
            results.sort(key=lambda x: x["similarity"], reverse=True)
            return results[:n_results]
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def delete_session_data(self) -> bool:
        """
        ì„¸ì…˜ ë°ì´í„° ì‚­ì œ (í´ë” ì „ì²´ ì‚­ì œ)
        
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            if self.ephemeral_dir.exists():
                shutil.rmtree(self.ephemeral_dir)
                print(f"âœ… ì„¸ì…˜ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {self.ephemeral_dir}")
            return True
        except Exception as e:
            print(f"âŒ ì„¸ì…˜ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_association_count(self) -> int:
        """
        ì €ì¥ëœ ì—°ìƒ ë‹¨ì–´ ê°œìˆ˜ ì¡°íšŒ
        
        Returns:
            int: í•­ëª© ê°œìˆ˜
        """
        return len(self.data.get("associations", []))
    
    def get_all_associations(self) -> List[str]:
        """
        ì €ì¥ëœ ëª¨ë“  ì—°ìƒ ë‹¨ì–´ í…ìŠ¤íŠ¸ ë°˜í™˜ (ì„ë² ë”© ì œì™¸)
        
        Returns:
            List[str]: ì—°ìƒ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        """
        return [item["text"] for item in self.data.get("associations", [])]
    
    def filter_trend_keywords(self, trend_keywords: List[str], top_k: int = 10) -> List[str]:
        """
        íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©ì Q3 í‚¤ì›Œë“œ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        
        ì‚¬ìš©ì í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ íŠ¸ë Œë“œë§Œ ì„ ë³„í•˜ì—¬ ì ë¦¼ ë°©ì§€
        
        Args:
            trend_keywords: íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            top_k: ì„ ë³„í•  ìƒìœ„ ê°œìˆ˜
            
        Returns:
            List[str]: í•„í„°ë§ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œ
        """
        if not self.data["associations"]:
            print("âš ï¸ ì‚¬ìš©ì í‚¤ì›Œë“œê°€ ì—†ì–´ íŠ¸ë Œë“œ í•„í„°ë§ ë¶ˆê°€")
            return trend_keywords[:top_k]
        
        if not trend_keywords:
            return []
        
        # 1. ì‚¬ìš©ì í‚¤ì›Œë“œë“¤ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚° (ê¸°ì¤€ì )
        user_embeddings = [item["embedding"] for item in self.data["associations"]]
        avg_user_embedding = np.mean(user_embeddings, axis=0).tolist()
        
        # 2. ê° íŠ¸ë Œë“œ í‚¤ì›Œë“œì™€ ì‚¬ìš©ì ê¸°ì¤€ì  ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        trend_scores = []
        for trend_kw in trend_keywords:
            try:
                trend_embedding = self.embed_text(trend_kw)
                similarity = self._cosine_similarity(avg_user_embedding, trend_embedding)
                trend_scores.append({
                    "keyword": trend_kw,
                    "similarity": similarity
                })
            except Exception as e:
                print(f"âš ï¸ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì„ë² ë”© ì‹¤íŒ¨: {trend_kw} - {e}")
                continue
        
        # 3. ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ kê°œ ì„ ë³„
        trend_scores.sort(key=lambda x: x["similarity"], reverse=True)
        filtered = [ts["keyword"] for ts in trend_scores[:top_k]]
        
        print(f"\nğŸ” íŠ¸ë Œë“œ í•„í„°ë§ ê²°ê³¼:")
        print(f"   ì›ë³¸: {len(trend_keywords)}ê°œ â†’ í•„í„°ë§: {len(filtered)}ê°œ")
        for i, ts in enumerate(trend_scores[:top_k], 1):
            print(f"   {i}. {ts['keyword']} (ìœ ì‚¬ë„: {ts['similarity']:.4f})")
        
        return filtered


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================

def cleanup_old_sessions(max_age_seconds: int = 3600) -> int:
    """
    ì˜¤ë˜ëœ ì„¸ì…˜ ë°ì´í„° ì •ë¦¬
    
    Args:
        max_age_seconds: ì´ ì‹œê°„(ì´ˆ)ë³´ë‹¤ ì˜¤ë˜ëœ ì„¸ì…˜ ì‚­ì œ
        
    Returns:
        int: ì‚­ì œëœ ì„¸ì…˜ ìˆ˜
    """
    import time
    
    current_file = Path(__file__).resolve()
    module_dir = current_file.parent
    ephemeral_base = module_dir / "data" / "ephemeral"
    
    if not ephemeral_base.exists():
        return 0
    
    deleted_count = 0
    current_time = time.time()
    
    for session_dir in ephemeral_base.iterdir():
        if session_dir.is_dir():
            # ë””ë ‰í† ë¦¬ ìˆ˜ì • ì‹œê°„ í™•ì¸
            dir_mtime = session_dir.stat().st_mtime
            age = current_time - dir_mtime
            
            if age > max_age_seconds:
                try:
                    shutil.rmtree(session_dir)
                    print(f"ğŸ§¹ ì˜¤ë˜ëœ ì„¸ì…˜ ì‚­ì œ: {session_dir.name}")
                    deleted_count += 1
                except Exception as e:
                    print(f"âŒ ì„¸ì…˜ ì‚­ì œ ì‹¤íŒ¨ ({session_dir.name}): {e}")
    
    print(f"âœ… ì´ {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ì„¸ì…˜ì„ ì²­ì†Œí–ˆìŠµë‹ˆë‹¤.")
    return deleted_count


def get_session_data_path(session_id: str) -> Path:
    """
    ì„¸ì…˜ ë°ì´í„° ê²½ë¡œ ë°˜í™˜
    
    Args:
        session_id: ì„¸ì…˜ ID
        
    Returns:
        Path: ì„¸ì…˜ ë°ì´í„° ê²½ë¡œ
    """
    current_file = Path(__file__).resolve()
    module_dir = current_file.parent
    return module_dir / "data" / "ephemeral" / session_id / "associations.json"


# ============================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================

if __name__ == "__main__":
    import sys
    sys.path.append(str(Path(__file__).resolve().parent))
    
    print("=" * 60)
    print("ì„ì‹œ RAG í…ŒìŠ¤íŠ¸ (JSON ê¸°ë°˜)")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ìš© ì„¸ì…˜ ID
    test_session_id = "test_session_001"
    
    # 1. Ephemeral RAG ì´ˆê¸°í™”
    print("\n[1] Ephemeral RAG ì´ˆê¸°í™”")
    ephemeral_rag = EphemeralRAG(session_id=test_session_id)
    
    # 2. Q1 ëª©ì  ì„¤ì •
    print("\n[2] Q1 ëª©ì  ì„¤ì •")
    q1_purpose = "ê±´ê°• ê´€ë¦¬ ëª¨ë°”ì¼ ì•± ì•„ì´ë””ì–´"
    print(f"    Q1: {q1_purpose}")
    
    # 3. Q3 ììœ ì—°ìƒ ì¶”ê°€
    print("\n[3] Q3 ììœ ì—°ìƒ ì¶”ê°€")
    q3_associations = [
        "ìš´ë™", "ì‹ë‹¨", "ìˆ˜ë©´", "ìŠ¤íŠ¸ë ˆì¹­", "ìš”ê°€",
        "ì¹¼ë¡œë¦¬", "ê±¸ìŒ ìˆ˜", "ì‹¬ë°•ìˆ˜", "ëª…ìƒ", "ë¬¼ ë§ˆì‹œê¸°"
    ]
    ephemeral_rag.add_associations(q3_associations)
    print(f"    ì¶”ê°€ëœ ì—°ìƒ ë‹¨ì–´: {len(q3_associations)}ê°œ")
    print(f"    ì €ì¥ëœ í•­ëª© ìˆ˜: {ephemeral_rag.get_association_count()}ê°œ")
    
    # 4. Q1ê³¼ Q3 ê°„ ìœ ì‚¬ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
    print("\n[4] Q1-Q3 ìœ ì‚¬ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ")
    top_keywords = ephemeral_rag.extract_keywords_by_similarity(q1_purpose, top_k=5)
    
    # 5. íŠ¹ì • ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
    print("\n[5] íŠ¹ì • ì¿¼ë¦¬ë¡œ ê²€ìƒ‰")
    search_query = "ìš´ë™ê³¼ ê´€ë ¨ëœ ê¸°ëŠ¥"
    search_results = ephemeral_rag.search_associations(search_query, n_results=3)
    print(f"    ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
    for i, result in enumerate(search_results, 1):
        print(f"    {i}. {result['document']} (ìœ ì‚¬ë„: {result['similarity']:.4f})")
    
    # 6. JSON íŒŒì¼ í™•ì¸
    print("\n[6] JSON íŒŒì¼ í™•ì¸")
    print(f"    ğŸ“„ {ephemeral_rag.json_path}")
    print(f"    ğŸ‘€ íŒŒì¼ì„ ì§ì ‘ ì—´ì–´ì„œ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    
    # 7. ì„¸ì…˜ ë°ì´í„° ì‚­ì œ
    print("\n[7] ì„¸ì…˜ ë°ì´í„° ì‚­ì œ")
    ephemeral_rag.delete_session_data()
    
    print("\n" + "=" * 60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)
