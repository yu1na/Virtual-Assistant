"""
Judge í´ë˜ìŠ¤ - GPT-5ë¥¼ ì‚¬ìš©í•œ í‰ê°€
"""

import json
from openai import OpenAI
from typing import Dict, List, Optional
from dotenv import load_dotenv
import os

from .models import EvaluationScore
from .prompts import build_judge_prompt, JUDGE_SYSTEM_PROMPT

load_dotenv()


class BrainstormingJudge:
    """
    ë¸Œë ˆì¸ìŠ¤í† ë° ê²°ê³¼ í‰ê°€ì
    
    GPT-5ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ì•„ì´ë””ì–´ë¥¼ 5ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        model: str = "gpt-5",
        temperature: float = 1.0,
        api_key: Optional[str] = None
    ):
        """
        Args:
            model: OpenAI ëª¨ë¸ëª… (ê¸°ë³¸: gpt-5)
            temperature: ìƒì„± ì˜¨ë„ (GPT-5ëŠ” 1.0ë§Œ ì§€ì›)
            api_key: OpenAI API Key (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ)
        """
        self.model = model
        self.temperature = temperature  # GPT-5ëŠ” ì‚¬ìš© ì•ˆ í•¨ (ê¸°ë³¸ê°’ 1.0)
        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        
        print(f"âœ… BrainstormingJudge ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - Model: {self.model}")
        print(f"   - Temperature: {self.temperature} (GPT-5ëŠ” ê¸°ë³¸ê°’ë§Œ ì§€ì›)")
    
    def evaluate(
        self,
        question: str,
        answer: str,
        permanent_rag_docs: List[str],
        ephemeral_keywords: List[str]
    ) -> EvaluationScore:
        """
        ë¸Œë ˆì¸ìŠ¤í† ë° ê²°ê³¼ í‰ê°€
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸ (Q1 ëª©ì )
            answer: AI ë‹µë³€ (ìƒì„±ëœ ì•„ì´ë””ì–´ ì „ë¬¸)
            permanent_rag_docs: Permanent RAG ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            ephemeral_keywords: Ephemeral RAG í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            EvaluationScore: í‰ê°€ ê²°ê³¼
        """
        
        # Judge í”„ë¡¬í”„íŠ¸ ìƒì„±
        user_prompt = build_judge_prompt(
            question=question,
            answer=answer,
            permanent_rag_docs=permanent_rag_docs,
            ephemeral_keywords=ephemeral_keywords
        )
        
        # GPT-5 í˜¸ì¶œ
        try:
            # GPT-5ëŠ” temperature ì¡°ì • ë¶ˆê°€ (ê¸°ë³¸ê°’ 1ë§Œ ì§€ì›)
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": JUDGE_SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"}
            )
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content
            scores_dict = json.loads(content)
            
            # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            score = EvaluationScore(**scores_dict)
            
            return score
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì‘ë‹µ ë‚´ìš©: {content}")
            raise
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def evaluate_batch(
        self,
        test_cases: List[Dict]
    ) -> List[EvaluationScore]:
        """
        ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¼ê´„ í‰ê°€
        
        Args:
            test_cases: í‰ê°€í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            List[EvaluationScore]: í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i, case in enumerate(test_cases):
            print(f"ğŸ” í‰ê°€ ì¤‘... ({i+1}/{len(test_cases)})")
            
            score = self.evaluate(
                question=case["question"],
                answer=case["answer"],
                permanent_rag_docs=case.get("permanent_rag_docs", []),
                ephemeral_keywords=case.get("ephemeral_keywords", [])
            )
            
            results.append(score)
        
        print(f"âœ… ì¼ê´„ í‰ê°€ ì™„ë£Œ: {len(results)}ê°œ")
        return results

