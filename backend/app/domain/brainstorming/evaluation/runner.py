"""
í‰ê°€ ìë™ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Usage:
    # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
    python -m backend.app.domain.brainstorming.evaluation.runner
    
    # íŠ¹ì • í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë§Œ ì‹¤í–‰
    python -m backend.app.domain.brainstorming.evaluation.runner --case-id tc001
"""

import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import List
import statistics

# ê²½ë¡œ ì„¤ì •
current_file = Path(__file__).resolve()
module_dir = current_file.parent
project_root = module_dir.parents[4]
sys.path.insert(0, str(project_root))

# ë¸Œë ˆì¸ìŠ¤í† ë° ëª¨ë“ˆ import
from backend.app.domain.brainstorming.session_manager import SessionManager
from backend.app.domain.brainstorming.ephemeral_rag import EphemeralRAG

# ChromaDB ë° OpenAI import
import chromadb
from chromadb.config import Settings as ChromaSettings
from openai import OpenAI
from dotenv import load_dotenv
import os

# í‰ê°€ ëª¨ë“ˆ import
from .judge import BrainstormingJudge
from .models import SingleRunResult, TestCaseResult, EvaluationSummary, TestCaseInput
from .criteria import WEIGHTS
from .test_cases import ALL_TEST_CASES, get_test_case_by_id

# íŠ¸ë Œë“œ ê²€ìƒ‰ ëª¨ë“ˆ
import asyncio
from backend.app.domain.brainstorming.search.naver_news import NaverNewsSearcher
from backend.app.domain.brainstorming.search.duckduckgo import DuckDuckGoSearcher
from backend.app.domain.brainstorming.search.naver_datalab import NaverDataLabSearcher

load_dotenv()


class EvaluationRunner:
    """í‰ê°€ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.session_manager = SessionManager()
        self.judge = BrainstormingJudge(model="gpt-5", temperature=1.0)
        
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.llm_model = os.getenv("LLM_MODEL", "gpt-4o")
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-large")
        
        # Permanent RAG ChromaDB
        brainstorming_path = module_dir.parent
        persist_directory = str(brainstorming_path / "data" / "chroma")
        
        self.chroma_client = chromadb.PersistentClient(
            path=persist_directory,
            settings=ChromaSettings(anonymized_telemetry=False)
        )
        
        try:
            self.permanent_collection = self.chroma_client.get_collection(
                name="brainstorming_techniques"
            )
            print("âœ… Permanent RAG ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  Permanent RAG ì»¬ë ‰ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.permanent_collection = None
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.results_dir = module_dir / "results"
        self.results_dir.mkdir(exist_ok=True)
        
        # íŠ¸ë Œë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        try:
            self.trend_searcher = NaverNewsSearcher()
            print("âœ… ë„¤ì´ë²„ íŠ¸ë Œë“œ ê²€ìƒ‰ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  íŠ¸ë Œë“œ ê²€ìƒ‰ ì´ˆê¸°í™” ì‹¤íŒ¨ (ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
            self.trend_searcher = None
        
        # DuckDuckGo ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        try:
            self.duckduckgo_searcher = DuckDuckGoSearcher()
            print("âœ… DuckDuckGo íŠ¸ë Œë“œ ê²€ìƒ‰ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  DuckDuckGo ì´ˆê¸°í™” ì‹¤íŒ¨ (ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
            self.duckduckgo_searcher = None
        
        # ë„¤ì´ë²„ ë°ì´í„°ë© ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        try:
            self.datalab_searcher = NaverDataLabSearcher()
            print("âœ… ë„¤ì´ë²„ ë°ì´í„°ë© ê²€ìƒ‰ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  ë„¤ì´ë²„ ë°ì´í„°ë© ì´ˆê¸°í™” ì‹¤íŒ¨ (ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
            self.datalab_searcher = None
    
    def run_single_test(self, test_case: dict, run_number: int) -> SingleRunResult:
        """
        ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (1íšŒ)
        
        Args:
            test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            run_number: ì‹¤í–‰ ë²ˆí˜¸ (1, 2, 3)
        
        Returns:
            SingleRunResult: ì‹¤í–‰ ê²°ê³¼
        """
        print(f"\n{'='*60}")
        print(f"ğŸ”„ ì‹¤í–‰ {run_number}/3: {test_case['name']}")
        print(f"{'='*60}")
        
        # 1. ì„¸ì…˜ ìƒì„±
        session_id = self.session_manager.create_session()
        print(f"âœ… ì„¸ì…˜ ìƒì„±: {session_id}")
        
        session = self.session_manager.get_session(session_id)
        
        try:
            # 2. Q1 ëª©ì  ì…ë ¥
            purpose = test_case["q1_purpose"]
            self.session_manager.update_session(session_id, {
                'q1_purpose': purpose
            })
            print(f"âœ… Q1 ëª©ì  ì…ë ¥ ì™„ë£Œ")
            
            # 3. Q3 ììœ ì—°ìƒ ì…ë ¥ + Ephemeral RAG ìƒì„±
            associations = test_case["q3_associations"]
            
            ephemeral_rag = EphemeralRAG(session_id=session_id)
            
            ephemeral_rag.add_associations(associations)
            
            self.session_manager.update_session(session_id, {
                'q3_associations': associations,
                'ephemeral_rag_initialized': True
            })
            print(f"âœ… Q3 ììœ ì—°ìƒ ì…ë ¥ + Ephemeral RAG ìƒì„± ì™„ë£Œ")
            
            # 4. Ephemeral RAG í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords_data = ephemeral_rag.extract_keywords_by_similarity(
                purpose=purpose,
                top_k=5
            )
            extracted_keywords = [kw['keyword'] for kw in keywords_data]
            print(f"âœ… Ephemeral RAG í‚¤ì›Œë“œ ì¶”ì¶œ: {extracted_keywords}")
            
            # 5. Permanent RAG ê²€ìƒ‰
            rag_docs = []
            if self.permanent_collection:
                purpose_embedding = self.openai_client.embeddings.create(
                    input=purpose,
                    model=self.embedding_model
                ).data[0].embedding
                
                results = self.permanent_collection.query(
                    query_embeddings=[purpose_embedding],
                    n_results=3
                )
                
                if results and results.get('documents') and results['documents'][0]:
                    rag_docs = results['documents'][0]
                    print(f"âœ… Permanent RAG ê²€ìƒ‰ ì™„ë£Œ: {len(rag_docs)}ê°œ ë¬¸ì„œ")
            
            # 6. [NEW] íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê²€ìƒ‰ (ë„¤ì´ë²„ + DuckDuckGo + ë°ì´í„°ë©)
            trend_keywords = []
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤
            if self.trend_searcher:
                try:
                    naver_keywords = asyncio.run(
                        self.trend_searcher.extract_trend_keywords(purpose, num_articles=5)
                    )
                    if naver_keywords:
                        print(f"   âœ… ë„¤ì´ë²„: {len(naver_keywords)}ê°œ ë°œê²¬")
                        trend_keywords.extend(naver_keywords)
                except Exception as e:
                    print(f"âš ï¸  ë„¤ì´ë²„ íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # DuckDuckGo
            if self.duckduckgo_searcher:
                try:
                    ddg_keywords = asyncio.run(
                        self.duckduckgo_searcher.extract_trend_keywords(purpose, num_articles=5)
                    )
                    if ddg_keywords:
                        print(f"   âœ… DuckDuckGo: {len(ddg_keywords)}ê°œ ë°œê²¬")
                        trend_keywords.extend(ddg_keywords)
                except Exception as e:
                    print(f"âš ï¸  DuckDuckGo ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ë„¤ì´ë²„ ë°ì´í„°ë©
            if self.datalab_searcher:
                try:
                    datalab_keywords = asyncio.run(
                        self.datalab_searcher.extract_trend_keywords(purpose)
                    )
                    if datalab_keywords:
                        print(f"   âœ… ë„¤ì´ë²„ ë°ì´í„°ë©: {len(datalab_keywords)}ê°œ ë°œê²¬")
                        trend_keywords.extend(datalab_keywords)
                except Exception as e:
                    print(f"âš ï¸  ë„¤ì´ë²„ ë°ì´í„°ë© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ì¤‘ë³µ ì œê±°
            trend_keywords = list(dict.fromkeys(trend_keywords))
            print(f"âœ… ì´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ: {len(trend_keywords)}ê°œ")
            
            # 7. [NEW] íŠ¸ë Œë“œ í‚¤ì›Œë“œ í•„í„°ë§ (ì‚¬ìš©ì í‚¤ì›Œë“œ ê¸°ì¤€)
            if trend_keywords:
                trend_keywords = ephemeral_rag.filter_trend_keywords(trend_keywords, top_k=10)
                print(f"âœ… í•„í„°ë§ í›„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ: {len(trend_keywords)}ê°œ")
            
            # 8. ì•„ì´ë””ì–´ ìƒì„± (ì‹¤ì œ API ë¡œì§ ë³µì œ)
            rag_context = "\n\n".join(rag_docs) if rag_docs else ""
            
            # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¬¸ìì—´ ìƒì„±
            trend_str = ", ".join(trend_keywords) if trend_keywords else "ì—†ìŒ"
            
            prompt = f"""ì‚¬ìš©ìê°€ "{purpose}"ì— ëŒ€í•œ ì•„ì´ë””ì–´ë¥¼ ì›í•©ë‹ˆë‹¤.

ã€ğŸ”´ í•µì‹¬: ì‚¬ìš©ì ë¸Œë ˆì¸ìŠ¤í† ë° í‚¤ì›Œë“œ (ë¹„ì¤‘ 80%)ã€‘
{', '.join(extracted_keywords)}

â€» ìœ„ í‚¤ì›Œë“œëŠ” ì‚¬ìš©ìê°€ ì§ì ‘ ë– ì˜¬ë¦° ê²ƒì…ë‹ˆë‹¤. ì´ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì•„ì´ë””ì–´ë¥¼ êµ¬ì„±í•˜ì„¸ìš”.

ã€ğŸ”µ ì°¸ê³ : ìµœì‹  íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ë¹„ì¤‘ 20%)ã€‘
{trend_str}

â€» íŠ¸ë Œë“œëŠ” ì°¸ê³ ë§Œ í•˜ì„¸ìš”. ì‚¬ìš©ì í‚¤ì›Œë“œê°€ í•µì‹¬ì…ë‹ˆë‹¤.

ã€ì ìš© ê°€ëŠ¥í•œ ë¸Œë ˆì¸ìŠ¤í† ë° ê¸°ë²•ã€‘
{rag_context}

---
**ğŸš¨ í•„ìˆ˜ ê·œì¹™**

1. **ë°˜ë“œì‹œ 3ê°œ ì•„ì´ë””ì–´ ìƒì„±**

2. **ë¹„ì¤‘ ì¤€ìˆ˜**: ì‚¬ìš©ì í‚¤ì›Œë“œ 80% + íŠ¸ë Œë“œ 20%
   - ì•„ì´ë””ì–´ì˜ í•µì‹¬ì€ ë°˜ë“œì‹œ ì‚¬ìš©ì í‚¤ì›Œë“œì—ì„œ ë‚˜ì™€ì•¼ í•¨
   - íŠ¸ë Œë“œëŠ” ì‹œì˜ì„± ì¶”ê°€ìš©ìœ¼ë¡œë§Œ ì‚´ì§ í™œìš©

3. **í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€**
   âŒ íŠ¹ì • ë„êµ¬/ì„œë¹„ìŠ¤ì˜ ê¸°ëŠ¥ì„ ë‹¨ì •ì§“ê¸° ê¸ˆì§€
   âŒ í†µê³„, ë¹„ìš©, ì‹œì¥ê·œëª¨ ì§€ì–´ë‚´ê¸° ê¸ˆì§€
   âœ… ëª¨ë¥´ëŠ” ê±´ "í™•ì¸ í•„ìš”"ë¡œ í‘œì‹œ

4. **í˜„ì‹¤ì  ì‹¤í–‰ ê°€ëŠ¥**: ë©°ì¹ ~ëª‡ ì£¼ ë‚´ ì‹œì‘ ê°€ëŠ¥í•œ ê²ƒë§Œ

---
**ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œ 3ê°œ ì‘ì„±)**:

---
ì•„ì´ë””ì–´ ì œëª©: [ì œëª©]

ì£¼ì œ: [ì–´ë–¤ ë¬¸ì œ/ë‹ˆì¦ˆë¥¼ í•´ê²°í•˜ëŠ”ì§€]

ì‹¤í–‰ ë°©í–¥: [ë¬´ì—‡ì„ í• ì§€ - êµ¬ì²´ì  ë„êµ¬ë‚˜ ìˆ˜ì¹˜ ë‹¨ì • ê¸ˆì§€, ë°©í–¥ì„±ë§Œ]

í™•ì¸ í•„ìš” ì‚¬í•­: [ì‹¤í–‰ ì „ ì¡°ì‚¬í•´ë´ì•¼ í•  ê²ƒë“¤]

ê¸°ëŒ€íš¨ê³¼: [ì˜ˆìƒ ê²°ê³¼ - ìˆ«ì ë‹¨ì • ê¸ˆì§€]

ì ìš©ëœ ê¸°ë²•: [ê¸°ë²•ëª…]
---

**âš ï¸ ë°˜ë“œì‹œ ìœ„ í˜•ì‹ìœ¼ë¡œ 3ê°œ ëª¨ë‘ ì‘ì„±í•˜ì„¸ìš”!**"""
            
            idea_response = self.openai_client.chat.completions.create(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ê¸°íšìì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1500
            )
            
            ideas_text = idea_response.choices[0].message.content.strip()
            print(f"âœ… ì•„ì´ë””ì–´ ìƒì„± ì™„ë£Œ")
            
            # 9. ì•„ì´ë””ì–´ íŒŒì‹± (ìƒˆ í˜•ì‹)
            ideas = []
            current_idea = {}
            current_field = None
            
            for line in ideas_text.split('\n'):
                line = line.strip()
                if not line:
                    continue
                
                if line.startswith('---'):
                    if current_idea and current_idea.get('title'):
                        ideas.append(current_idea)
                    current_idea = {}
                    current_field = None
                elif line.startswith('ì•„ì´ë””ì–´ ì œëª©:') or line.startswith('ì œëª©:'):
                    current_idea['title'] = line.split(':', 1)[1].strip()
                    current_field = None
                elif line.startswith('ì£¼ì œ:'):
                    current_idea['subject'] = line.split(':', 1)[1].strip()
                    current_field = 'subject'
                elif line.startswith('ì‹¤í–‰ ë°©í–¥:'):
                    current_idea['direction'] = line.split(':', 1)[1].strip()
                    current_field = 'direction'
                elif line.startswith('í™•ì¸ í•„ìš” ì‚¬í•­:') or line.startswith('í™•ì¸ í•„ìš”:'):
                    current_idea['check_needed'] = line.split(':', 1)[1].strip()
                    current_field = 'check_needed'
                elif line.startswith('ê¸°ëŒ€íš¨ê³¼:') or line.startswith('ê¸°ëŒ€ íš¨ê³¼:'):
                    current_idea['expected_effect'] = line.split(':', 1)[1].strip()
                    current_field = 'expected_effect'
                elif line.startswith('ì ìš©ëœ ê¸°ë²•:') or line.startswith('ê¸°ë²•:'):
                    current_idea['technique'] = line.split(':', 1)[1].strip()
                    current_field = None
                # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜
                elif line.startswith('- ì„¤ëª…:') or line.startswith('ì„¤ëª…:'):
                    current_idea['description'] = line.split(':', 1)[1].strip()
                    current_field = 'description'
                elif line.startswith('ì•„ì´ë””ì–´') and ':' in line:
                    if current_idea and current_idea.get('title'):
                        ideas.append(current_idea)
                    title = line.split(':', 1)[1].strip()
                    current_idea = {'title': title}
                    current_field = None
                elif current_field and line:
                    if current_field in current_idea:
                        current_idea[current_field] += ' ' + line
                    else:
                        current_idea[current_field] = line
            
            if current_idea and current_idea.get('title'):
                ideas.append(current_idea)
            
            # 10. SWOT ë¶„ì„ ì¶”ê°€
            for idea in ideas:
                idea_content = f"""
ì œëª©: {idea.get('title', '')}
ì£¼ì œ: {idea.get('subject', idea.get('description', ''))}
ì‹¤í–‰ ë°©í–¥: {idea.get('direction', '')}
"""
                swot_prompt = f"""**ì—­í• **: í˜„ì‹¤ì ì¸ ê¸°íšì

**ì•„ì´ë””ì–´**: {idea_content}

**ìš”êµ¬ì‚¬í•­**:
1. ì´ ì•„ì´ë””ì–´ì— ëŒ€í•œ **SWOT ë¶„ì„** ìˆ˜í–‰
2. **í˜„ì‹¤ì  ê´€ì **ì—ì„œ ë¶„ì„ (ì‚¬ìš©ìì˜ ìƒí™©: ê°œì¸/ì†Œê·œëª¨ íŒ€/íšŒì‚¬)
3. ê° í•­ëª©ì„ **1-2ì¤„**ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
4. **í—ˆìœ„ ë°ì´í„° ì ˆëŒ€ ê¸ˆì§€** (ëª¨ë¥´ë©´ "ì¡°ì‚¬ í•„ìš”")

**ì¶œë ¥ í˜•ì‹**:
Strengths (ê°•ì ):
- [ê°•ì  1]
- [ê°•ì  2]

Weaknesses (ì•½ì ):
- [ì•½ì  1]
- [ì•½ì  2]

Opportunities (ê¸°íšŒ):
- [ê¸°íšŒ 1]
- [ê¸°íšŒ 2]

Threats (ìœ„í˜‘):
- [ìœ„í˜‘ 1]
- [ìœ„í˜‘ 2]
"""
                
                swot_response = self.openai_client.chat.completions.create(
                    model=self.llm_model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ í˜„ì‹¤ì ì¸ ê¸°íšìì…ë‹ˆë‹¤."},
                        {"role": "user", "content": swot_prompt}
                    ],
                    temperature=0.6,
                    max_tokens=500
                )
                
                idea['analysis'] = swot_response.choices[0].message.content.strip()
            
            print(f"âœ… SWOT ë¶„ì„ ì™„ë£Œ: {len(ideas)}ê°œ ì•„ì´ë””ì–´")
            
            # 11. ìµœì¢… í…ìŠ¤íŠ¸ í¬ë§·
            final_ideas_text = ""
            for i, idea in enumerate(ideas, 1):
                final_ideas_text += f"ğŸ“Œ ì•„ì´ë””ì–´ {i}: {idea.get('title', '')}\n\n"
                if idea.get('subject'):
                    final_ideas_text += f"ì£¼ì œ: {idea.get('subject')}\n"
                if idea.get('direction'):
                    final_ideas_text += f"ì‹¤í–‰ ë°©í–¥: {idea.get('direction')}\n"
                if idea.get('check_needed'):
                    final_ideas_text += f"í™•ì¸ í•„ìš”: {idea.get('check_needed')}\n"
                if idea.get('expected_effect'):
                    final_ideas_text += f"ê¸°ëŒ€íš¨ê³¼: {idea.get('expected_effect')}\n"
                if idea.get('description'):
                    final_ideas_text += f"ì„¤ëª…: {idea.get('description')}\n"
                final_ideas_text += f"\nğŸ“Š SWOT ë¶„ì„:\n\n{idea.get('analysis', '')}\n\n"
            
            # 12. Judge í‰ê°€
            print(f"ğŸ” Judge í‰ê°€ ì¤‘... (GPT-5)")
            scores = self.judge.evaluate(
                question=purpose,
                answer=final_ideas_text,
                permanent_rag_docs=rag_docs,
                ephemeral_keywords=extracted_keywords
            )
            
            weighted_total = scores.weighted_average(WEIGHTS)
            
            print(f"âœ… í‰ê°€ ì™„ë£Œ:")
            print(f"   - rag_utilization (RAG í™œìš©ë„): {scores.rag_utilization}/10")
            print(f"   - completeness (ë‹µë³€ ì™„ì„±ë„): {scores.completeness}/10")
            print(f"   - relevance (ì§ˆë¬¸-ë‹µë³€ ì—°ê´€ë„): {scores.relevance}/10")
            print(f"   - creativity (ì°½ì˜ì„±): {scores.creativity}/10")
            print(f"   - practicality (ì‹¤ìš©ì„±): {scores.practicality}/10")
            print(f"   - weighted_total (ê°€ì¤‘ í‰ê· ): {weighted_total}/10")
            
            # 13. ê²°ê³¼ ìƒì„±
            result = SingleRunResult(
                run_number=run_number,
                session_id=session_id,
                ideas_text=final_ideas_text,
                ideas_count=len(ideas),
                permanent_rag_docs=rag_docs,
                ephemeral_keywords=extracted_keywords,
                scores=scores,
                weighted_total=weighted_total
            )
            
            return result
            
        finally:
            # 14. ì„¸ì…˜ ì •ë¦¬
            self.session_manager.delete_session(session_id)
            print(f"âœ… ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")
    
    def run_test_case(self, test_case: dict) -> TestCaseResult:
        """
        í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ (3íšŒ ë°˜ë³µ)
        
        Args:
            test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        
        Returns:
            TestCaseResult: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì „ì²´ ê²°ê³¼
        """
        print(f"\n{'#'*60}")
        print(f"ğŸš€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹œì‘: {test_case['name']} ({test_case['id']})")
        print(f"{'#'*60}")
        
        runs = []
        
        # 3íšŒ ì‹¤í–‰
        for i in range(1, 4):
            run_result = self.run_single_test(test_case, i)
            runs.append(run_result)
        
        # í‰ê·  ê³„ì‚°
        avg_rag = sum(r.scores.rag_utilization for r in runs) / 3
        avg_comp = sum(r.scores.completeness for r in runs) / 3
        avg_rel = sum(r.scores.relevance for r in runs) / 3
        avg_cre = sum(r.scores.creativity for r in runs) / 3
        avg_prac = sum(r.scores.practicality for r in runs) / 3
        avg_weighted = sum(r.weighted_total for r in runs) / 3
        
        average_scores = {
            "rag_utilization": round(avg_rag, 2),
            "completeness": round(avg_comp, 2),
            "relevance": round(avg_rel, 2),
            "creativity": round(avg_cre, 2),
            "practicality": round(avg_prac, 2),
        }
        
        # í‘œì¤€í¸ì°¨ ê³„ì‚°
        weighted_scores = [r.weighted_total for r in runs]
        std_dev = round(statistics.stdev(weighted_scores) if len(weighted_scores) > 1 else 0.0, 2)
        
        result = TestCaseResult(
            test_case_id=test_case["id"],
            test_case_name=test_case["name"],
            runs=runs,
            average_scores=average_scores,
            average_weighted_total=round(avg_weighted, 2),
            std_deviation=std_dev
        )
        
        # í•œê¸€ ë ˆì´ë¸” ë§¤í•‘
        label_map = {
            "rag_utilization": "RAG í™œìš©ë„",
            "completeness": "ë‹µë³€ ì™„ì„±ë„",
            "relevance": "ì§ˆë¬¸-ë‹µë³€ ì—°ê´€ë„",
            "creativity": "ì°½ì˜ì„±",
            "practicality": "ì‹¤ìš©ì„±"
        }
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì™„ë£Œ: {test_case['name']}")
        print(f"{'='*60}")
        print(f"í‰ê·  ì ìˆ˜:")
        for key, value in average_scores.items():
            korean_label = label_map.get(key, key)
            print(f"  - {key} ({korean_label}): {value}/10")
        print(f"weighted_total (ê°€ì¤‘ í‰ê· ): {result.average_weighted_total}/10")
        print(f"std_deviation (í‘œì¤€í¸ì°¨): {std_dev} (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ)")
        
        return result
    
    def save_result(self, result: TestCaseResult):
        """ê²°ê³¼ JSON ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{result.test_case_id}_result.json"
        filepath = self.results_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result.model_dump(), f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ê²°ê³¼ ì €ì¥: {filepath}")
    
    def run_all(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰"""
        print(f"\n{'#'*60}")
        print(f"ğŸ¯ ì „ì²´ í‰ê°€ ì‹œì‘")
        print(f"{'#'*60}")
        print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(ALL_TEST_CASES)}ê°œ")
        
        all_results = []
        
        for test_case in ALL_TEST_CASES:
            result = self.run_test_case(test_case)
            self.save_result(result)
            all_results.append(result)
        
        # ì „ì²´ ìš”ì•½
        overall_avg = sum(r.average_weighted_total for r in all_results) / len(all_results)
        
        summary = EvaluationSummary(
            test_cases=all_results,
            overall_average=round(overall_avg, 2),
            model_info={
                "worker_model": self.llm_model,
                "judge_model": self.judge.model,
                "embedding_model": self.embedding_model
            }
        )
        
        # ìš”ì•½ ì €ì¥
        summary_filename = f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        summary_filepath = self.results_dir / summary_filename
        
        with open(summary_filepath, 'w', encoding='utf-8') as f:
            json.dump(summary.model_dump(), f, ensure_ascii=False, indent=2)
        
        print(f"\n{'#'*60}")
        print(f"âœ… ì „ì²´ í‰ê°€ ì™„ë£Œ!")
        print(f"{'#'*60}")
        print(f"overall_average (ì „ì²´ í‰ê·  ì ìˆ˜): {overall_avg}/10")
        print(f"summary_file (ìš”ì•½ íŒŒì¼): {summary_filepath}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë¸Œë ˆì¸ìŠ¤í† ë° í‰ê°€ ìë™ ì‹¤í–‰")
    parser.add_argument(
        "--case-id",
        type=str,
        help="íŠ¹ì • í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ID (ì˜ˆ: tc001). ìƒëµ ì‹œ ì „ì²´ ì‹¤í–‰"
    )
    
    args = parser.parse_args()
    
    runner = EvaluationRunner()
    
    if args.case_id:
        # íŠ¹ì • ì¼€ì´ìŠ¤ë§Œ ì‹¤í–‰
        test_case = get_test_case_by_id(args.case_id)
        result = runner.run_test_case(test_case)
        runner.save_result(result)
    else:
        # ì „ì²´ ì‹¤í–‰
        runner.run_all()


if __name__ == "__main__":
    main()

