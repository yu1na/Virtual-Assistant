"""
RAG ê²€ìƒ‰ ëª¨ë“ˆ (LangChain ê¸°ë°˜)

LangChain ì²´ì¸ê³¼ LangSmithë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

from typing import List, Optional, Dict, Any
import time
import os
import json
import datetime

# LangChain imports
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI

# LangSmith ì„¤ì •
from langsmith import traceable

from .config import rag_config
from .vector_store import VectorStore
from .schemas import QueryRequest, QueryResponse, RetrievedChunk
from .utils import get_logger
from .evaluator import RAGEvaluator

logger = get_logger(__name__)


class RAGRetriever:
    """RAG ê¸°ë°˜ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± (LangChain ì²´ì¸ ì‚¬ìš©)"""
    
    def __init__(self, collection_name: Optional[str] = None):
        self.config = rag_config
        self.vector_store = VectorStore(collection_name)
        
        # LangSmith ì„¤ì •
        if self.config.LANGSMITH_TRACING and self.config.LANGSMITH_API_KEY:
            os.environ["LANGCHAIN_TRACING_V2"] = "true"
            os.environ["LANGCHAIN_API_KEY"] = self.config.LANGSMITH_API_KEY
            os.environ["LANGCHAIN_PROJECT"] = self.config.LANGSMITH_PROJECT
            logger.info(f"LangSmith ì¶”ì  í™œì„±í™”: {self.config.LANGSMITH_PROJECT}")
        
        # Lazy loading: LLMì„ ì‹¤ì œ ì‚¬ìš© ì‹œì—ë§Œ ë¡œë“œ
        self._llm = None
        self._rag_chain = None
        self._evaluator = None
        
        logger.info("RAGRetriever ì´ˆê¸°í™” ì™„ë£Œ (LLM lazy loading)")

    @property
    def evaluator(self):
        """Evaluator lazy loading"""
        if self._evaluator is None:
            self._evaluator = RAGEvaluator()
        return self._evaluator
    
    @property
    def llm(self):
        """LLM lazy loading"""
        if self._llm is None:
            self._llm = ChatOpenAI(
                model=self.config.OPENAI_MODEL,
                temperature=self.config.OPENAI_TEMPERATURE,
                max_tokens=self.config.OPENAI_MAX_TOKENS,
                api_key=self.config.OPENAI_API_KEY
            )
            logger.info(f"LLM ë¡œë“œ ì™„ë£Œ: {self.config.OPENAI_MODEL}")
        return self._llm
    
    @property
    def prompt_template(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        return ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ê·œì¹™ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

1. **ë‹µë³€ ì›ì¹™**:
   - ì œê³µëœ ë¬¸ì„œ(Context)ì— ìˆëŠ” ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
   - **ì‚¬ìš©ìê°€ ë¬»ëŠ” ì •ë³´ê°€ ë¬¸ì„œì— ëª…í™•íˆ ì—†ë”ë¼ë„, ë¬¸ë§¥ìƒ ìœ ì¶”í•  ìˆ˜ ìˆê±°ë‚˜ ê´€ë ¨ëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì´ë¥¼ ì°¾ì•„ì„œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.**
   - ì•„ì˜ˆ ê´€ë ¨ ë‚´ìš©ì´ ì—†ì„ ë•Œë§Œ "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

2. **Markdown í•„ìˆ˜**: ê°€ë…ì„±ì„ ìœ„í•´ Markdownì„ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
   - **ëª¨ë“  ëª©ë¡(ê¸€ë¨¸ë¦¬ ê¸°í˜¸)ê³¼ ì†Œì œëª©(`###`) ì•ë’¤ì—ëŠ” ë°˜ë“œì‹œ ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ë‘ ë²ˆ ì‚¬ìš©í•˜ì—¬ ë¹ˆ ì¤„ì„ ë§Œë“œì„¸ìš”.**
   - í•µì‹¬ ë‚´ìš©ì€ **ë³¼ë“œì²´**ë¡œ ê°•ì¡°í•©ë‹ˆë‹¤.

3. **ê°„ê²°ì„±**: ë¶ˆí•„ìš”í•œ ì„œë¡ ì„ ë¹¼ê³  í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. **ì–¸ì–´**: í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""),
            ("user", """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

{context}

ì§ˆë¬¸: {query}

ë‹µë³€:""")
        ])
    
    @property
    def rag_chain(self):
        """RAG ì²´ì¸ lazy loading"""
        if self._rag_chain is None:
            self._rag_chain = self._build_rag_chain()
            logger.info("RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ")
        return self._rag_chain
    
    def _build_rag_chain(self):
        """LangChain íŒŒì´í”„ ì—°ì‚°ì(|)ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ êµ¬ì„±"""
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° ë™ì  threshold í•„í„°ë§
        @traceable(name="retrieve_and_filter")
        def retrieve_and_filter(inputs: Dict[str, Any]) -> Dict[str, Any]:
            """ë¬¸ì„œ ê²€ìƒ‰ ë° ë™ì  threshold ê¸°ë°˜ í•„í„°ë§ (í›„ë³´êµ° í™•ëŒ€ + í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… + ìµœì†Œ ë³´ì¥)"""
            query = inputs["query"]
            top_k = inputs.get("top_k", self.config.RAG_TOP_K)
            
            logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘: '{query}' (Top-{top_k})")
            
            # 1ë‹¨ê³„: ë„‰ë„‰í•˜ê²Œ ë§ì´ ê°€ì ¸ì˜¤ê¸° (fetch_k=20)
            fetch_k = 20
            results = self.vector_store.search(query, fetch_k)
            
            # ê²°ê³¼ ë³€í™˜
            candidates = []
            all_similarities = []
            
            # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
            if not results:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            elif not results.get('documents') or not results['documents']:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            elif not results['documents'][0]:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            else:
                doc_list = results['documents'][0]
                similarity_list = results.get('distances', [[]])[0] if results.get('distances') else []
                meta_list = results.get('metadatas', [[]])[0] if results.get('metadatas') else []
                
                logger.info(f"í›„ë³´êµ° ê²€ìƒ‰ ê²°ê³¼: {len(doc_list)}ê°œ ë¬¸ì„œ, {len(similarity_list)}ê°œ ìœ ì‚¬ë„ ì ìˆ˜")
                
                # ëª¨ë“  í›„ë³´êµ° ìˆ˜ì§‘
                for i in range(len(doc_list)):
                    if i < len(similarity_list):
                        similarity_score = float(similarity_list[i])
                    else:
                        similarity_score = 0.0
                    
                    all_similarities.append(similarity_score)
                    
                    metadata = meta_list[i] if i < len(meta_list) else {}
                    chunk = RetrievedChunk(
                        text=doc_list[i],
                        metadata=metadata,
                        score=similarity_score
                    )
                    candidates.append(chunk)
            
            # 2ë‹¨ê³„: í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚° ë° ë¶€ìŠ¤íŒ…
            def apply_keyword_boosting(chunks: List[RetrievedChunk], query_text: str) -> List[RetrievedChunk]:
                """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ ë¶€ìŠ¤íŒ…"""
                query_words = set(query_text.lower().split())
                scored_candidates = []
                
                for chunk in chunks:
                    keyword_score = 0.0
                    chunk_text_lower = chunk.text.lower()
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ ì²´í¬)
                    for word in query_words:
                        if len(word) > 2 and word in chunk_text_lower:
                            keyword_score += 0.02  # í‚¤ì›Œë“œë‹¹ +0.02 ë¶€ìŠ¤íŒ…
                    
                    # ë¶€ìŠ¤íŒ…ëœ ì ìˆ˜ë¡œ ìƒˆ ì²­í¬ ìƒì„±
                    boosted_score = chunk.score + keyword_score
                    boosted_chunk = RetrievedChunk(
                        text=chunk.text,
                        metadata=chunk.metadata,
                        score=boosted_score
                    )
                    scored_candidates.append(boosted_chunk)
                    
                    if keyword_score > 0:
                        logger.debug(f"í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…: {chunk.metadata.get('filename', 'Unknown')} "
                                   f"(ê¸°ë³¸: {chunk.score:.4f}, ë¶€ìŠ¤íŒ…: +{keyword_score:.4f}, ìµœì¢…: {boosted_score:.4f})")
                
                return scored_candidates
            
            scored_candidates = apply_keyword_boosting(candidates, query)
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            scored_candidates.sort(key=lambda x: x.score, reverse=True)
            
            # 3ë‹¨ê³„: ë™ì  threshold ê³„ì‚°
            if all_similarities:
                # ìµœê³  ì ìˆ˜ì™€ í‰ê·  ì ìˆ˜ ê³„ì‚°
                max_similarity = max(all_similarities)
                avg_similarity = sum(all_similarities) / len(all_similarities)
                
                # ë™ì  threshold: ìµœê³  ì ìˆ˜ì™€ í‰ê· ì˜ ì¤‘ê°„ê°’, min~max ë²”ìœ„ ë‚´ë¡œ ì œí•œ
                dynamic_threshold = (max_similarity + avg_similarity) / 2
                dynamic_threshold = max(
                    self.config.RAG_MIN_SIMILARITY_THRESHOLD,
                    min(dynamic_threshold, self.config.RAG_MAX_SIMILARITY_THRESHOLD)
                )
                
                logger.info(f"ë™ì  threshold ê³„ì‚°: max={max_similarity:.4f}, avg={avg_similarity:.4f}, "
                           f"threshold={dynamic_threshold:.4f} (ë²”ìœ„: {self.config.RAG_MIN_SIMILARITY_THRESHOLD}~{self.config.RAG_MAX_SIMILARITY_THRESHOLD})")
            else:
                dynamic_threshold = self.config.RAG_MIN_SIMILARITY_THRESHOLD
                logger.warning(f"ìœ ì‚¬ë„ ì—†ìŒ, ê¸°ë³¸ threshold ì‚¬ìš©: {dynamic_threshold}")
            
            # 4ë‹¨ê³„: Threshold ì ìš© (ë‹¨, ìµœì†Œ 3ê°œëŠ” ë³´ì¥)
            final_results = []
            for chunk in scored_candidates:
                if chunk.score > dynamic_threshold:
                    final_results.append(chunk)
                    logger.debug(f"  âœ“ Threshold í†µê³¼: {chunk.metadata.get('filename', 'Unknown')}, "
                               f"í˜ì´ì§€: {chunk.metadata.get('page_number', '?')}, "
                               f"ì ìˆ˜: {chunk.score:.4f} > {dynamic_threshold:.4f}")
            
            # ì•ˆì „ì¥ì¹˜: Thresholdë¥¼ ë„˜ì€ ê²Œ ë„ˆë¬´ ì ìœ¼ë©´, ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ìµœì†Œ 3ê°œ ì±„ìš°ê¸°
            min_guaranteed = 3
            if len(final_results) < min_guaranteed:
                logger.warning(f"Threshold í†µê³¼ ì²­í¬ê°€ {len(final_results)}ê°œë¡œ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                             f"ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ìµœì†Œ {min_guaranteed}ê°œ ë³´ì¥í•©ë‹ˆë‹¤.")
                final_results = scored_candidates[:min_guaranteed]
                logger.info(f"ìµœì†Œ ë³´ì¥ ì ìš©: {len(final_results)}ê°œ ì²­í¬ ì„ íƒ")
            
            logger.info(f"Threshold í•„í„°ë§ ê²°ê³¼: {len(final_results)}ê°œ ì²­í¬ (í›„ë³´êµ° {len(candidates)}ê°œ ì¤‘)")
            
            # 5ë‹¨ê³„: ê°™ì€ í˜ì´ì§€ì˜ ì²­í¬ë“¤ì„ ë¬¶ì–´ì„œ í•©ì¹˜ê¸°
            from collections import defaultdict
            page_groups = defaultdict(list)
            
            for chunk in final_results:
                filename = chunk.metadata.get('filename', 'Unknown')
                page_num = chunk.metadata.get('page_number', 0)
                key = (filename, page_num)
                
                chunk_index = chunk.metadata.get('chunk_index', 0)
                
                page_groups[key].append({
                    'chunk': chunk,
                    'score': chunk.score,
                    'chunk_index': chunk_index
                })
            
            # ê° í˜ì´ì§€ ê·¸ë£¹ ë‚´ì—ì„œ chunk_index ìˆœì„œë¡œ ì •ë ¬
            merged_chunks = []
            for (filename, page_num), group_chunks in page_groups.items():
                # chunk_index ìˆœì„œë¡œ ì •ë ¬
                group_chunks.sort(key=lambda x: x['chunk_index'])
                
                # ê°™ì€ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ í•©ì¹˜ê¸°
                merged_text_parts = []
                max_score = max(g['score'] for g in group_chunks)
                
                for gc in group_chunks:
                    merged_text_parts.append(gc['chunk'].text)
                
                merged_text = "\n".join(merged_text_parts)
                
                # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë˜, ì ìˆ˜ëŠ” ê·¸ë£¹ ë‚´ ìµœê³  ì ìˆ˜ ì‚¬ìš©
                first_chunk = group_chunks[0]['chunk']
                merged_chunk = RetrievedChunk(
                    text=merged_text,
                    metadata=first_chunk.metadata,
                    score=max_score
                )
                
                merged_chunks.append({
                    'chunk': merged_chunk,
                    'score': max_score
                })
            
            # ì ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
            merged_chunks.sort(key=lambda x: x['score'], reverse=True)
            
            # 6ë‹¨ê³„: ê·¸ ì¤‘ì—ì„œ Top-5 ìë¥´ê¸°
            final_chunks = [mc['chunk'] for mc in merged_chunks[:top_k]]
            
            logger.info(f"ìµœì¢… ì„ íƒ: {len(final_chunks)}ê°œ í˜ì´ì§€ ê·¸ë£¹ "
                      f"(í›„ë³´êµ° {len(candidates)}ê°œ â†’ Threshold í†µê³¼ {len(final_results)}ê°œ â†’ "
                      f"ë³‘í•© {len(merged_chunks)}ê°œ â†’ Top-{top_k} {len(final_chunks)}ê°œ)")
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            for i, chunk in enumerate(final_chunks, 1):
                context_parts.append(f"[ë¬¸ì„œ {i}]")
                context_parts.append(f"íŒŒì¼: {chunk.metadata.get('filename', 'Unknown')}")
                context_parts.append(f"í˜ì´ì§€: {chunk.metadata.get('page_number', 'Unknown')}")
                context_parts.append(f"ë‚´ìš©:\n{chunk.text}")
                context_parts.append("")
            
            context = "\n".join(context_parts) if context_parts else "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return {
                "query": query,
                "context": context,
                "retrieved_chunks": final_chunks,
                "top_k": top_k,
                "dynamic_threshold": dynamic_threshold
            }
        
        # 2. ë‹µë³€ ìƒì„±
        @traceable(name="generate_answer")
        def generate_answer(inputs: Dict[str, Any]) -> Dict[str, Any]:
            """LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
            query = inputs["query"]
            context = inputs["context"]
            retrieved_chunks = inputs["retrieved_chunks"]
            
            if not retrieved_chunks:
                logger.warning("ê²€ìƒ‰ëœ ì²­í¬ê°€ ì—†ìŒ - ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜")
                return {
                    **inputs,
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # LangChain ì²´ì¸ ì‹¤í–‰: prompt | llm | parser
            answer = (
                self.prompt_template 
                | self.llm 
                | StrOutputParser()
            ).invoke({
                "query": query,
                "context": context
            })
            
            return {
                **inputs,
                "answer": answer
            }
        
        # LangChain ì²´ì¸ êµ¬ì„± (íŒŒì´í”„ ì—°ì‚°ì ì‚¬ìš©)
        chain = (
            RunnablePassthrough()
            | RunnableLambda(retrieve_and_filter)
            | RunnableLambda(generate_answer)
        )
        
        return chain
    
    @traceable(
        name="rag_query_full",
        metadata={
            "component": "RAG System",
            "version": "1.0"
        }
    )
    def query(self, request: QueryRequest) -> QueryResponse:
        """
        ì§ˆì˜ì‘ë‹µ ì „ì²´ í”„ë¡œì„¸ìŠ¤ (ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ RAG ë˜ëŠ” LLM ë‹¨ë… ì‚¬ìš©)
        
        Args:
            request: ì§ˆì˜ì‘ë‹µ ìš”ì²­
            
        Returns:
            QueryResponse: ì§ˆì˜ì‘ë‹µ ì‘ë‹µ
        """
        start_time = time.time()
        
        try:
            # ë¬¸ì„œ ê²€ìƒ‰ í•„ìš”: RAG ì‹¤í–‰
            logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ í•„ìš”: '{request.query}' -> RAG ì‹¤í–‰")
            
            # LangChain ì²´ì¸ ì‹¤í–‰ (ë™ì  thresholdëŠ” ìë™ ê³„ì‚°)
            result = self.rag_chain.invoke({
                "query": request.query,
                "top_k": request.top_k or self.config.RAG_TOP_K
            })
            
            answer = result["answer"]
            retrieved_chunks = result["retrieved_chunks"]
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ: Small talk ì‚¬ìš©í•˜ì§€ ì•Šê³  "ì •ë³´ ì—†ìŒ" ë©”ì‹œì§€
            if not retrieved_chunks:
                logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: '{request.query}' -> ì •ë³´ ë¶€ì¡± ë©”ì‹œì§€ ë°˜í™˜")
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # LangSmithì— ë©”íƒ€ë°ì´í„° ì „ë‹¬ì„ ìœ„í•´ dictë¡œ ë³€í™˜
            response = QueryResponse(
                query=request.query,
                answer=answer,
                retrieved_chunks=retrieved_chunks,
                processing_time=processing_time,
                model_used=self.config.OPENAI_MODEL
            )
            
            # ì‹¤ì‹œê°„ í‰ê°€ ìˆ˜í–‰ (í„°ë¯¸ë„ ì¶œë ¥ìš©)
            # ì‹¤ì‹œê°„ í‰ê°€ ìˆ˜í–‰ (í„°ë¯¸ë„ ì¶œë ¥ìš©) - ë¹„í™œì„±í™” (ì†ë„ ê°œì„  ë° í† í° ì ˆì•½)
            # try:
            #     print("\n" + "="*50)
            #     print("ğŸ” ì‹¤ì‹œê°„ RAG ë‹µë³€ í‰ê°€ ìˆ˜í–‰ ì¤‘...")
            #     # Ground Truth ì¡°íšŒ (í‰ê°€ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©)
            #     ground_truth = self.evaluator.lookup_ground_truth(request.query)
                
            #     eval_result = self.evaluator.evaluate_single(
            #         question=request.query,
            #         answer=answer,
            #         context="\n".join([chunk.text for chunk in retrieved_chunks]),
            #         ground_truth=ground_truth
            #     )
            #     print(f"  - ì •í™•ì„± (Faithfulness): {eval_result.get('faithfulness_score')}ì ")
            #     print(f"  - ì™„ì „ì„± (Completeness): {eval_result.get('completeness_score')}ì ")
            #     print(f"  - ì—°ê´€ì„± (Answer Relevancy): {eval_result.get('answer_relevancy_score')}ì ")
            #     print(f"  - ì •ë°€ë„ (Context Precision): {eval_result.get('context_precision_score')}ì ")
            #     print(f"  - ì¼ì¹˜ë„ (Answer Correctness): {eval_result.get('answer_correctness_score')}ì ")
                
            #     # ê²°ê³¼ JSON ì €ì¥
            #     try:
            #         timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    
            #         # ì ˆëŒ€ ê²½ë¡œ ê³„ì‚°: backend/data/HR_RAG/HR_RAG_result
            #         current_dir = os.path.dirname(os.path.abspath(__file__))
            #         # backend/app/domain/rag/HR -> ... -> backend
            #         backend_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_dir))))
            #         result_dir = os.path.join(backend_dir, "data", "HR_RAG", "HR_RAG_result")
                    
            #         os.makedirs(result_dir, exist_ok=True)
                    
            #         result_file = os.path.join(result_dir, f"evaluation_{timestamp}.json")
                    
            #         # ì €ì¥í•  ë°ì´í„° êµ¬ì„±
            #         save_data = {
            #             "timestamp": timestamp,
            #             "query": request.query,
            #             "answer": answer,
            #             "retrieved_chunks": [
            #                 {
            #                     "filename": chunk.metadata.get("filename", "Unknown"),
            #                     "page": chunk.metadata.get("page_number", "?"),
            #                     "score": chunk.score,
            #                     "text": chunk.text
            #                 } for chunk in retrieved_chunks
            #             ],
            #             "ground_truth": ground_truth,
            #             "evaluation": eval_result
            #         }
                    
            #         with open(result_file, 'w', encoding='utf-8') as f:
            #             json.dump(save_data, f, ensure_ascii=False, indent=4)
                        
            #         logger.info(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_file}")
            #         print(f"  - ê²°ê³¼ íŒŒì¼ ì €ì¥: {result_file}")
                    
            #     except Exception as save_e:
            #         logger.error(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {save_e}")
            #         print(f"  - ê²°ê³¼ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_e}")

            #     print("="*50 + "\n")
                    
            # except Exception as eval_e:
            #     logger.warning(f"ì‹¤ì‹œê°„ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {eval_e}")
            #     print(f"âŒ ì‹¤ì‹œê°„ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {eval_e}")
            #     print("="*50 + "\n")
            
            # LangSmith ë©”íƒ€ë°ì´í„° ë¡œê¹…
            from langsmith import traceable
            from langsmith.run_helpers import get_current_run_tree
            
            try:
                run_tree = get_current_run_tree()
                if run_tree:
                    run_tree.extra = {
                        "retrieved_chunks_count": len(retrieved_chunks),
                        "chunks": [
                            {
                                "filename": chunk.metadata.get("filename", "Unknown"),
                                "page_number": chunk.metadata.get("page_number", 0),
                                "score": chunk.score
                            }
                            for chunk in retrieved_chunks
                        ],
                        "processing_time": processing_time,
                        "model": self.config.OPENAI_MODEL
                    }
            except Exception as e:
                logger.warning(f"LangSmith ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: {e}")
            
            return response
            
        except Exception as e:
            logger.exception("ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜")
            processing_time = time.time() - start_time
            
            return QueryResponse(
                query=request.query,
                answer=f"ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                retrieved_chunks=[],
                processing_time=processing_time,
                model_used=self.config.OPENAI_MODEL
            )
    
