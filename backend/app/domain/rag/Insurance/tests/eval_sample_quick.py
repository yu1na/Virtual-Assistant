#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Insurance RAG + LLM í‰ê°€ (ì‹¤í—˜ ê°€ëŠ¥í•œ ë²„ì „)
ìž„ë² ë”© ëª¨ë¸, retriever í•˜ì´ë¸Œë¦¬ë“œ ë¹„ìœ¨ ë“±ì„ ì‰½ê²Œ ë³€ê²½ ê°€ëŠ¥
"""

import os
import json
import random
import argparse
from pathlib import Path
from typing import List, Dict, Any

env_path = Path('/Users/doyeonkim/Documents/GitHub/Virtual-Assistant/.env')
if env_path.exists():
    with open(env_path) as f:
        for line in f:
            if line.startswith('OPENAI_API_KEY='):
                os.environ['OPENAI_API_KEY'] = line.split('=')[1].strip().strip('"\'')
                break

import chromadb
from openai import OpenAI
import numpy as np
from tqdm import tqdm

# ===== ì„¤ì •ê°’ (CLIë¡œë„ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥) =====
class Config:
    # ê²½ë¡œ
    CHROMA_DB_PATH = 'backend/app/domain/rag/Insurance/chroma_db'
    COLLECTION_NAME = 'insurance_manual'
    QA_FILE_PATH = 'backend/app/domain/rag/Insurance/tests/qa_filtered_300.json'
    
    # ìž„ë² ë”©/LLM ëª¨ë¸
    EMBEDDING_MODEL = "text-embedding-3-small"  # ì‹¤í—˜í•  ë•Œ: text-embedding-3-large ë“±ìœ¼ë¡œ ë³€ê²½
    LLM_MODEL = "gpt-4o-mini"
    
    # ê²€ìƒ‰ ì„¤ì •
    TOP_K = 5
    HYBRID_RATIO = 1.0  # 1.0 = ìˆœìˆ˜ ë²¡í„°, 0.0 = ìˆœìˆ˜ í‚¤ì›Œë“œ, 0.5 = í•˜ì´ë¸Œë¦¬ë“œ 50/50
    
    # í‰ê°€ ì„¤ì •
    SAMPLE_SIZE = 30
    RANDOM_SEED = 42
    
    @classmethod
    def get_output_path(cls):
        """ì‹¤í—˜ ì„¤ì •ì— ë”°ë¥¸ ì¶œë ¥ ê²½ë¡œ ìžë™ ìƒì„±"""
        exp_name = f"emb_{cls.EMBEDDING_MODEL.split('-')[-1]}_hybrid_{cls.HYBRID_RATIO}_topk_{cls.TOP_K}"
        return f'backend/app/domain/rag/Insurance/tests/results/eval_{exp_name}.json'
    
    @classmethod
    def update_from_args(cls, args):
        """CLI ì¸ìžë¡œ ì„¤ì • ì—…ë°ì´íŠ¸"""
        if args.embedding_model:
            cls.EMBEDDING_MODEL = args.embedding_model
        if args.hybrid_ratio is not None:
            cls.HYBRID_RATIO = args.hybrid_ratio
        if args.top_k:
            cls.TOP_K = args.top_k
        if args.sample_size:
            cls.SAMPLE_SIZE = args.sample_size

def parse_args():
    parser = argparse.ArgumentParser(description='Insurance RAG + LLM í‰ê°€')
    parser.add_argument('--embedding-model', help='ìž„ë² ë”© ëª¨ë¸ (ê¸°ë³¸: text-embedding-3-small)')
    parser.add_argument('--hybrid-ratio', type=float, help='í•˜ì´ë¸Œë¦¬ë“œ ë¹„ìœ¨ (0.0~1.0, ê¸°ë³¸: 1.0)')
    parser.add_argument('--top-k', type=int, help='Top-K ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--sample-size', type=int, help='í‰ê°€ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: 30)')
    return parser.parse_args()

print("[1] ë°ì´í„° ë¡œë“œ...")
args = parse_args()
Config.update_from_args(args)

print(f"âš™ï¸  ì‹¤í—˜ ì„¤ì •:")
print(f"  ìž„ë² ë”© ëª¨ë¸: {Config.EMBEDDING_MODEL}")
print(f"  í•˜ì´ë¸Œë¦¬ë“œ ë¹„ìœ¨: {Config.HYBRID_RATIO}")
print(f"  Top-K: {Config.TOP_K}")
print(f"  ìƒ˜í”Œ ìˆ˜: {Config.SAMPLE_SIZE}")
print()

client = chromadb.PersistentClient(path=Config.CHROMA_DB_PATH)
collection = client.get_collection(Config.COLLECTION_NAME)

with open(Config.QA_FILE_PATH, 'r', encoding='utf-8') as f:
    all_qa_data = json.load(f)

# ìƒ˜í”Œë§
random.seed(Config.RANDOM_SEED)
qa_data = random.sample(all_qa_data, min(Config.SAMPLE_SIZE, len(all_qa_data)))
print(f"âœ… {len(all_qa_data)}ê°œ ì¤‘ {Config.SAMPLE_SIZE}ê°œ ìƒ˜í”Œë§")

openai_client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))

def get_embedding(text: str) -> List[float]:
    """í…ìŠ¤íŠ¸ ìž„ë² ë”© ìƒì„±"""
    response = openai_client.embeddings.create(
        input=text[:8000],
        model=Config.EMBEDDING_MODEL
    )
    return response.data[0].embedding

def retrieve_context(question: str, top_k: int = None) -> List[str]:
    """ChromaDBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›)"""
    if top_k is None:
        top_k = Config.TOP_K
    
    query_emb = get_embedding(question)
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° + í‚¤ì›Œë“œ
    # í˜„ìž¬ëŠ” ë²¡í„°ë§Œ ì‚¬ìš© (HYBRID_RATIO=1.0)
    # ì¶”í›„ BM25 ë“± í‚¤ì›Œë“œ ê²€ìƒ‰ ì¶”ê°€ ê°€ëŠ¥
    if Config.HYBRID_RATIO == 1.0:
        # ìˆœìˆ˜ ë²¡í„° ê²€ìƒ‰
        search_results = collection.query(
            query_embeddings=[query_emb],
            n_results=top_k
        )
    else:
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (êµ¬í˜„ ì˜ˆì •)
        # bm25_results = keyword_search(question, alpha=1-HYBRID_RATIO)
        # vector_results = vector_search(query_emb, alpha=HYBRID_RATIO)
        # results = merge_results(bm25_results, vector_results)
        search_results = collection.query(
            query_embeddings=[query_emb],
            n_results=top_k
        )
    
    return search_results['documents'][0] if search_results['documents'] else []

def generate_answer(question: str, context_chunks: List[str]) -> str:
    """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
    context = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n{chunk[:300]}" for i, chunk in enumerate(context_chunks)])
    
    prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]"""
    
    try:
        response = openai_client.chat.completions.create(
            model=Config.LLM_MODEL,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=300
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[ì˜¤ë¥˜: {str(e)[:30]}]"

def calc_similarity(text1: str, text2: str) -> float:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        emb1 = np.array(get_embedding(text1))
        emb2 = np.array(get_embedding(text2))
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        return float(similarity)
    except:
        return 0.0

print(f"\n[2] í‰ê°€ ì‹¤í–‰ ì¤‘... ({Config.SAMPLE_SIZE}ê°œ ìƒ˜í”Œ)")
results = []

for qa_item in tqdm(qa_data, desc="í‰ê°€"):
    question = qa_item.get('question', '')
    ground_truth = qa_item.get('answer', '')
    section = qa_item.get('section', '')
    
    if not question or not ground_truth:
        continue
    
    try:
        context = retrieve_context(question)
        answer = generate_answer(question, context)
        
        if "[ì˜¤ë¥˜" in answer:
            continue
        
        similarity = calc_similarity(answer, ground_truth)
        
        results.append({
            'question': question[:100],
            'section': section[:60],
            'ground_truth': ground_truth[:150],
            'generated': answer[:150],
            'similarity': similarity
        })
    except:
        continue

print(f"\n[3] í†µê³„...")
sims = [r['similarity'] for r in results]

print("\n" + "="*70)
print(f"ðŸ“Š RAG + LLM í‰ê°€ ê²°ê³¼ ({Config.SAMPLE_SIZE} ìƒ˜í”Œ)")
print("="*70)
print(f"\nì‹¤í—˜ ì„¤ì •:")
print(f"  ìž„ë² ë”©: {Config.EMBEDDING_MODEL}")
print(f"  í•˜ì´ë¸Œë¦¬ë“œ: {Config.HYBRID_RATIO}")
print(f"  Top-K: {Config.TOP_K}")
print(f"\nì„±ê³¼:")
print(f"  í‰ê·  ìœ ì‚¬ë„: {np.mean(sims):.3f}")
print(f"  ì¤‘ì•™ê°’: {np.median(sims):.3f}")
print(f"  ë²”ìœ„: {np.min(sims):.3f} ~ {np.max(sims):.3f}")

print(f"\nìœ ì‚¬ë„ ë¶„í¬:")
print(f"  >= 0.5: {sum(1 for s in sims if s >= 0.5)}/{len(results)} ({sum(1 for s in sims if s >= 0.5)/len(results)*100:.0f}%)")
print(f"  >= 0.6: {sum(1 for s in sims if s >= 0.6)}/{len(results)} ({sum(1 for s in sims if s >= 0.6)/len(results)*100:.0f}%)")
print(f"  >= 0.7: {sum(1 for s in sims if s >= 0.7)}/{len(results)} ({sum(1 for s in sims if s >= 0.7)/len(results)*100:.0f}%)")

print(f"\nìƒ˜í”Œ ê²°ê³¼ (ìƒìœ„ 5ê°œ):")
sorted_results = sorted(results, key=lambda x: x['similarity'], reverse=True)
for i, r in enumerate(sorted_results[:5], 1):
    print(f"\n[{i}] ìœ ì‚¬ë„: {r['similarity']:.3f}")
    print(f"  Q: {r['question']}")
    print(f"  ì •ë‹µ: {r['ground_truth'][:100]}")
    print(f"  ìƒì„±: {r['generated'][:100]}")

# ì €ìž¥
output_path = Config.get_output_path()
Path(output_path).parent.mkdir(parents=True, exist_ok=True)

output = {
    'config': {
        'embedding_model': Config.EMBEDDING_MODEL,
        'llm_model': Config.LLM_MODEL,
        'top_k': Config.TOP_K,
        'hybrid_ratio': Config.HYBRID_RATIO,
        'sample_size': Config.SAMPLE_SIZE,
    },
    'summary': {
        'total': len(results),
        'avg_similarity': float(np.mean(sims)),
        'median_similarity': float(np.median(sims)),
        'std_similarity': float(np.std(sims)),
        'min_similarity': float(np.min(sims)),
        'max_similarity': float(np.max(sims)),
        'threshold_0.5': sum(1 for s in sims if s >= 0.5),
        'threshold_0.6': sum(1 for s in sims if s >= 0.6),
        'threshold_0.7': sum(1 for s in sims if s >= 0.7),
    },
    'results': results
}

with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(output, f, ensure_ascii=False, indent=2)

print(f"\nðŸ’¾ ì €ìž¥: {output_path}")
print("âœ… ì™„ë£Œ!")
