"""
HR RAG Loader
íšŒì‚¬ ë¬¸ì„œ(uploads/)ë¥¼ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥
"""

import sys
from pathlib import Path
from typing import List

from app.domain.rag.HR.vector_store import VectorStore
from app.domain.rag.HR.document_converter import DocumentConverter
from app.domain.rag.HR.pdf_processor import PDFProcessor
from app.domain.rag.HR.schemas import ProcessedDocument, ProcessedContent, ContentType
from app.domain.rag.HR.config import rag_config
from app.core.config import settings


def init_hr_rag() -> bool:
    """
    HR RAG ChromaDB ì´ˆê¸°í™”
    
    - uploads/ í´ë”ì˜ PDF/TXT íŒŒì¼ì„ ì½ì–´ì„œ ì²˜ë¦¬
    - HR embedderë¡œ ì„ë² ë”© ìƒì„±
    - hr_documents ì»¬ë ‰ì…˜ì— ì €ì¥
    
    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    print("\nğŸ“‹ [HR RAG] ì´ˆê¸°í™” ì²´í¬...")
    
    try:
        # 1. ê²½ë¡œ ì„¤ì •
        base_dir = Path(__file__).parent.parent.parent.parent  # Virtual-Assistant ë£¨íŠ¸
        uploads_dir = base_dir / "backend" / "internal_docs" / "uploads"
        
        print(f"   ğŸ“‚ ì—…ë¡œë“œ í´ë”: {uploads_dir}")
        
        # ì—…ë¡œë“œ í´ë” í™•ì¸
        if not uploads_dir.exists():
            print(f"   âš ï¸  ì—…ë¡œë“œ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {uploads_dir}")
            return False
        
        # 2. VectorStore ì´ˆê¸°í™” (hr_documents ì»¬ë ‰ì…˜)
        vector_store = VectorStore(collection_name="hr_documents")
        
        # 3. ì´ë¯¸ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        current_count = vector_store.count_documents()
        if current_count > 0:
            print(f"   âœ… ì´ë¯¸ ì¡´ì¬ ({current_count}ê°œ ì²­í¬) - ìŠ¤í‚µ")
            return True
        
        # 4. ì—…ë¡œë“œ í´ë”ì˜ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        pdf_files = list(uploads_dir.glob("*.pdf"))
        txt_files = list(uploads_dir.glob("*.txt"))
        all_files = pdf_files + txt_files
        
        if not all_files:
            print(f"   âš ï¸  ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {uploads_dir}")
            return False
        
        print(f"   ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼: {len(all_files)}ê°œ (PDF: {len(pdf_files)}, TXT: {len(txt_files)})")
        
        # 5. DocumentConverter ì´ˆê¸°í™”
        converter = DocumentConverter()
        pdf_processor = PDFProcessor()
        
        # 6. ê° íŒŒì¼ ì²˜ë¦¬
        total_chunks = 0
        processed_files = []
        
        for file_path in all_files:
            try:
                print(f"   ğŸ“– ì²˜ë¦¬ ì¤‘: {file_path.name}...")
                
                # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
                if file_path.suffix.lower() == ".pdf":
                    # PDF ì²˜ë¦¬
                    processed_doc = pdf_processor.process_pdf(str(file_path))
                    # document_converterì—ì„œ processed_doc.filenameì„ document_idë¡œ ì‚¬ìš©
                elif file_path.suffix.lower() == ".txt":
                    # TXT íŒŒì¼ ì²˜ë¦¬
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text_content = f.read()
                    
                    # ProcessedContent ìƒì„± (metadata í¬í•¨)
                    from app.domain.rag.HR.schemas import DocumentMetadata
                    content = ProcessedContent(
                        page_number=1,
                        content_type=ContentType.TEXT,
                        text=text_content,
                        metadata=DocumentMetadata(
                            filename=file_path.name,
                            page_number=1,
                            content_type=ContentType.TEXT
                        )
                    )
                    
                    # ProcessedDocument ìƒì„±
                    # document_converterì—ì„œ filenameì„ document_idë¡œ ì‚¬ìš©
                    processed_doc = ProcessedDocument(
                        filename=file_path.name,
                        total_pages=1,
                        contents=[content],
                        file_path=str(file_path)
                    )
                else:
                    print(f"   âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}")
                    continue
                
                # ì²­í¬ ìƒì„±
                chunks = converter.create_chunks(processed_doc)
                
                if not chunks:
                    print(f"   âš ï¸  ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {file_path.name}")
                    continue
                
                # VectorStoreì— ì¶”ê°€
                added_count = vector_store.add_chunks(chunks, reuse_embeddings=False)
                total_chunks += added_count
                processed_files.append(file_path.name)
                
                print(f"   âœ… ì™„ë£Œ: {file_path.name} ({added_count}ê°œ ì²­í¬)")
                
            except Exception as e:
                print(f"   âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path.name}): {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # 7. ê²€ì¦
        final_count = vector_store.count_documents()
        print(f"   âœ… ì´ˆê¸°í™” ì™„ë£Œ! ({final_count}ê°œ ì²­í¬, {len(processed_files)}ê°œ íŒŒì¼)")
        
        if final_count != total_chunks:
            print(f"   âš ï¸  ê²½ê³ : ì˜ˆìƒ({total_chunks})ê³¼ ì‹¤ì œ({final_count}) ì²­í¬ ìˆ˜ ë¶ˆì¼ì¹˜")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("=" * 60)
    print("HR RAG ë¡œë” - ë…ë¦½ ì‹¤í–‰")
    print("=" * 60)
    
    result = init_hr_rag()
    
    print("\n" + "=" * 60)
    print(f"ê²°ê³¼: {'âœ… ì„±ê³µ' if result else 'âŒ ì‹¤íŒ¨'}")
    print("=" * 60)

